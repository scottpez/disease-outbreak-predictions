{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9739a20-82bb-4a61-8374-77b904970dd6",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae2c50-af2b-4462-8796-d04613439d5d",
   "metadata": {},
   "source": [
    "# EIOS machine learning to detect disease article signals for WHO-AFRO\n",
    "## Performed by Dr. Scott Pezanowski\n",
    "### 2023-05-01 to 2023-07-31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa1784-3c9c-40d8-9e35-76ce0999a5d3",
   "metadata": {},
   "source": [
    "1. Install Tensorflow and AutoKeras using the instructions found here https://autokeras.com/install/ .\n",
    "2. Run the commands below to install other Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba8580-cf9c-4994-a9a5-72e598d1afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install striprtf\n",
    "!python3 -m pip install scikit-learn\n",
    "!python3 -m pip install regex\n",
    "!python3 -m pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9929d318-5698-4a11-aeb5-4315ba6779d7",
   "metadata": {},
   "source": [
    "# Create the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7830dc8-eb85-4700-a331-b64b63de9d85",
   "metadata": {},
   "source": [
    "* The code belows assume that you exported from EIOS roughly equal numbers of articles previously labeled as sginals and articles not signals.\n",
    "* The more representative your articles are of all types of articles you are looking for, the better.\n",
    "* Articles will ideally come from different time periods, sources, be about different diseases, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1fa572-75ab-4961-8d56-be57f364a5e7",
   "metadata": {},
   "source": [
    "* Read in the data from the Doc files <br>\n",
    "* Note: They are actually not in MS Doc format. They are in RTF. So, we need the striprtf python library to convert RTF to TXT. <br>\n",
    "* The code assumes that for each set of documents, there are article signals in a directory named \"signal\" and articles that are not signals in a directory named \"not_signal.\" <br>\n",
    "* Here, I also set the variable max_chars which will be used later to truncate the full text to the first 4,000 characters for model training. Longer text will cause memory issues unless you have a lot of GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5798afd-0039-47a6-8826-30fe901fbc21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "import pandas as pd\n",
    "\n",
    "# Below is the primary path to your training data. You should have \"signal\" and \"not_signal\" subdirectories for your respective articles.\n",
    "data_path = '/mnt/h/My Drive/work/BrightWorldLabs/coreWork/consulting/2023/WHO-AFRO/data/signalprediction'\n",
    "dir_signal = 'signal'\n",
    "dir_not_signal = 'not_signal'\n",
    "min_chars = 40  # if the article is less than this number of characters, I will remove it below because it is likely not valuable and not a signal.\n",
    "max_chars = 4000  # used to truncate the article text because longer text causes computer memory problems and usually the first 4000 characters is enough to judge if it is a signal.\n",
    "max_words = 400  # not currently used\n",
    "\n",
    "# fields to parse. Some of these fields might be valuable to the model in the future. However, I only used full_text and translated_full_text.\n",
    "label_title = 'Title: '\n",
    "label_about_country = 'About country: '\n",
    "label_source = 'Source: '\n",
    "label_retrieval_date = 'Retrieval date: '\n",
    "label_source_type = 'Source type: '\n",
    "label_from_country = 'From country: '\n",
    "label_language = 'Language: '\n",
    "label_summary = 'Summary: '\n",
    "label_full_text = 'Full text: '\n",
    "label_translated_text = 'Translated text: '\n",
    "article_break = '(out of '\n",
    "\n",
    "articles = []\n",
    "article_columns = ['title', 'about_country', 'source', 'retrieval_date', 'source_type', 'from_country', 'language', 'summary', 'full_text', 'translated_text', 'signal_type']\n",
    "# loop through all articles and parse them line by line using their labels to place them in the appropriate variables.\n",
    "for f in glob.glob(data_path + '/**/' + '*.doc', recursive=True):\n",
    "    with open(f, 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "        rtf = file.read()\n",
    "        text = rtf_to_text(rtf)  # converts RTF format to plain text.\n",
    "        lines = text.split('\\n')\n",
    "        title = ''\n",
    "        about_country = ''\n",
    "        source = ''\n",
    "        retrieval_date = ''\n",
    "        source_type = ''\n",
    "        from_country = ''\n",
    "        language = ''\n",
    "        summary = ''\n",
    "        full_text = ''\n",
    "        translated_text = ''\n",
    "        for line in lines:\n",
    "            if article_break in line and '1(out of ' not in line:\n",
    "                # the next two lines creates the article label for machine learning. signal/not_signal\n",
    "                signal_type = 'signal'\n",
    "                if dir_not_signal in f:\n",
    "                    signal_type = 'not_signal'\n",
    "                article_arr = [\n",
    "                    title,\n",
    "                    about_country,\n",
    "                    source,\n",
    "                    retrieval_date.strftime('%s'),\n",
    "                    source_type,\n",
    "                    from_country,\n",
    "                    language,\n",
    "                    summary,\n",
    "                    full_text,\n",
    "                    translated_text,\n",
    "                    signal_type\n",
    "                ]\n",
    "                articles.append(article_arr)\n",
    "                title = ''\n",
    "                about_country = ''\n",
    "                source = ''\n",
    "                retrieval_date = ''\n",
    "                source_type = ''\n",
    "                from_country = ''\n",
    "                language = ''\n",
    "                summary = ''\n",
    "                full_text = ''\n",
    "                translated_text = ''\n",
    "            if label_title in line:\n",
    "                title = line.replace(label_title, '')\n",
    "            elif label_about_country in line:\n",
    "                tmp_arr = line.split('|')\n",
    "                about_country = tmp_arr[0].replace(label_about_country, '').strip()\n",
    "                source = tmp_arr[1].replace(label_source, '').strip()\n",
    "            elif label_retrieval_date in line:\n",
    "                retrieval_date = line.replace(label_retrieval_date, '').split('|')[0]\n",
    "                try:\n",
    "                    retrieval_date = datetime.strptime(retrieval_date + ' 2023', '%d %b %H:%M %Y')\n",
    "                except ValueError as ve:\n",
    "                    retrieval_date = datetime.now()\n",
    "            elif label_source_type in line:\n",
    "                tmp_arr = line.split('|')\n",
    "                source_type = tmp_arr[0].replace(label_source_type, '').strip()\n",
    "                from_country = tmp_arr[1].replace(label_from_country, '').strip()\n",
    "                language = tmp_arr[2].replace(label_language, '')\n",
    "            elif label_summary in line:\n",
    "                summary = line.replace(label_summary, '')\n",
    "            elif label_full_text in line:\n",
    "                tmp_arr = line.split('|')\n",
    "                full_text = tmp_arr[0].replace(label_full_text, '')\n",
    "                if len(full_text) > min_chars:\n",
    "                    full_text = full_text[:max_chars].encode(\n",
    "                        'utf-8', errors='ignore'\n",
    "                        ).decode('utf-8')\n",
    "            elif label_translated_text in line:\n",
    "                tmp_arr = line.split('|')\n",
    "                translated_text = tmp_arr[0].replace(label_translated_text, '')\n",
    "                if len(translated_text) > min_chars:\n",
    "                    translated_text = translated_text[:max_chars].encode(\n",
    "                        'utf-8', errors='ignore'\n",
    "                        ).decode('utf-8')\n",
    "        if len(title) > 0:\n",
    "            signal_type = 'signal'\n",
    "            if dir_not_signal in f:\n",
    "                signal_type = 'not_signal'\n",
    "            article_arr = [\n",
    "                title,\n",
    "                about_country,\n",
    "                source,\n",
    "                retrieval_date.strftime('%s'),\n",
    "                source_type,\n",
    "                from_country,\n",
    "                language,\n",
    "                summary,\n",
    "                full_text,\n",
    "                translated_text,\n",
    "                signal_type\n",
    "            ]\n",
    "            articles.append(article_arr)\n",
    "\n",
    "df = pd.DataFrame(articles, columns=article_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3117a3-f5f4-4f89-8817-3d6f515f9a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['full_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf32027-ae92-4b13-9b78-1c4b09448771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the full_text exists, use that for training. If not, use the translated_text.\n",
    "def set_final_text(row):\n",
    "    if row['language'] == 'English':\n",
    "        return row['full_text']\n",
    "    else:\n",
    "        return row['translated_text']\n",
    "\n",
    "\n",
    "df['final_text'] = df.apply(lambda row: set_final_text(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e8283-98a9-4d24-be17-a63aeb0281ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470dccb-c9d1-4735-9176-e77cec10ad58",
   "metadata": {},
   "source": [
    "## Downsample the not_signal class to match the number in the signal class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3688bf96-6f8b-40fc-8912-ddcbf85733c6",
   "metadata": {},
   "source": [
    "* Typically for binary classification, it is best to have equal amounts for each class.\n",
    "* Since I had more not_signals, I downsampled the not_signal class which means randomly removing articles until the amount matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b31cb-0aec-4a29-96de-c9270aabb028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final = df[df['final_text'] != ''] # drop those that are not in English and do not have a translation.\n",
    "df_signal = df_final[df_final['signal_type'] == 'signal']\n",
    "df_not_signal = df_final[df_final['signal_type'] == 'not_signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ad7f7-04d5-4ee7-8143-b2b45ee2c564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(df_signal))\n",
    "print(len(df_not_signal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c3d9d-9b36-47b8-af41-eaa79eb36d22",
   "metadata": {
    "tags": []
   },
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "df_not_signal = resample(df_not_signal,\n",
    "                         replace=True,\n",
    "                         n_samples=len(df_signal),\n",
    "                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5ea3f-645f-4f35-ba46-d473e979b77b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(df_signal))\n",
    "print(len(df_not_signal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b7f88a-f4a1-4f21-90ed-83c4d83d8df4",
   "metadata": {},
   "source": [
    "# Train the model using AutoKeras TextClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf95c5-9c2c-4814-a48d-8e8476d7d073",
   "metadata": {},
   "source": [
    "We will classify the documents as either signals or not signals. <br>\n",
    "Import necessary libraries for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8727f3-1264-44fd-8a35-19527d238a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987b11b-e381-423f-ac17-7a0b4318e58f",
   "metadata": {},
   "source": [
    "Split the data for training and testing. The default is an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ffd16-bf15-4955-8da4-a844925b0dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(pd.concat([df_signal, df_not_signal]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40725000-0d32-4216-b5c0-5fff236a5ff3",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273ec00-a13c-4197-9db5-7cb505a0a369",
   "metadata": {},
   "source": [
    "The libraries below contain puntuation and stopwords lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2300e-f6e1-40aa-9aa3-bce080cbffd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3b3eb-d728-4c34-82a0-028c32fdf1e9",
   "metadata": {},
   "source": [
    "My preprocessing steps include:\n",
    "\n",
    "* Removing non-utf characters like symbols.\n",
    "* Removing punctuation.\n",
    "* Removing stop words.\n",
    "* Set the label for signals as 1 (positive) and not_signals as 0 (negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68694df-b6d4-4541-8411-1acae780c171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "s_words = nltk.corpus.stopwords.words(\"english\")\n",
    "punc = string.punctuation\n",
    "punc += '’' + '‘' + '”' + '“'\n",
    "\n",
    "def remove_non_utf(txt):\n",
    "    return txt.encode(\n",
    "        'utf-8', errors='ignore'\n",
    "    ).decode('utf-8')\n",
    "\n",
    "\n",
    "def create_label(lbl):\n",
    "    if lbl == 'signal':\n",
    "        return 1\n",
    "    if lbl == 'not_signal':\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def remove_stop_words(txt):\n",
    "    new_txt = ''\n",
    "    word_len = 0\n",
    "    for sent in nltk.sent_tokenize(txt):\n",
    "        text_tokens = nltk.word_tokenize(sent)\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in s_words]\n",
    "        new_txt += ' '.join(tokens_without_sw) + '. '\n",
    "        word_len += len(tokens_without_sw)\n",
    "        if word_len >= max_words:\n",
    "            break\n",
    "    return new_txt\n",
    "\n",
    "def remove_punctuation(txt):\n",
    "    return \"\".join([w for w in txt if w not in punc])\n",
    "\n",
    "x_train = train_data['full_text']\n",
    "x_train = x_train.apply(remove_non_utf)\n",
    "x_train = x_train.apply(remove_stop_words)\n",
    "x_train = x_train.apply(remove_punctuation)\n",
    "\n",
    "y_train = train_data[article_columns[-1]].apply(create_label)\n",
    "x_test = test_data['full_text']\n",
    "x_test = x_test.apply(remove_non_utf)\n",
    "x_test = x_test.apply(remove_stop_words)\n",
    "x_test = x_test.apply(remove_punctuation)\n",
    "y_test = test_data[article_columns[-1]].apply(create_label)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(x_train.shape)  # (25000,)\n",
    "print(y_train.shape)  # (25000, 1)\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea4fe1-477b-433e-b793-48645fd6e5f4",
   "metadata": {},
   "source": [
    "## Load the data into AutoKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ce583-9313-4dbf-b426-b45e5cdc9461",
   "metadata": {},
   "source": [
    "* Use AutoKeras's TextClassifier to fit the model with 5 epochs.\n",
    "* With AutoML, the typical strategy is to first train with a small number of epochs allowing time for it to try lots of different model types.\n",
    "* Plus, usuaully it does not take long to decide which model is best.\n",
    "* Once we find the best model for the data, farther down we will train that model longer for better results.\n",
    "* I have a Nvidia RTX Titan GPU and I estimate the fit to find the best model took about 10 minutes.\n",
    "* Farther down, training the best model for longer took about 10 minutes.\n",
    "* The training time depends on your computer hardware and the amount of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695ffaf-d344-4afc-91da-46101974a609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the text classifier.\n",
    "clf = ak.TextClassifier(\n",
    "    overwrite=True, max_trials=4\n",
    ")  # It only tries 4 models as a quick demo. You can set max_trials higher to try many different models.\n",
    "# Feed the text classifier with training data.\n",
    "clf.fit(x_train, y_train, epochs=5)  # epochs=10\n",
    "# Predict with the best model.\n",
    "predicted_y = clf.predict(x_test)\n",
    "# Evaluate the best model with testing data.\n",
    "print(clf.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58e5c9-511f-45f2-9427-47a1f6e55f37",
   "metadata": {},
   "source": [
    "# Export the best performing model and try to train longer to obtain better results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11df395-e606-4e8a-bb16-08fa9d2768de",
   "metadata": {},
   "source": [
    "* In the step above, we found the best model.\n",
    "* Now, export that model to a standard Keras model.\n",
    "* Train the best model longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce74939-c8e4-4232-97f8-64a7d6132e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clf.export_model()\n",
    "print(best_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2425df5f-2818-421f-9d38-df432567f533",
   "metadata": {},
   "source": [
    "I used EarlyStopping to prevent overfitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce90d50-1901-4800-9a43-7f11a4c330ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "best_model.fit(x_train, y_train, epochs=100, batch_size=16, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289d8f9-cbd7-4899-8423-871f027520ef",
   "metadata": {},
   "source": [
    "Evalutae the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c84e93-9c43-4495-89c2-647cc8168497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(best_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1833f75-06ca-491a-a43e-666eebb21181",
   "metadata": {},
   "source": [
    "Get the actual probability predictions so that I can use them to sort the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d4238-f83a-492a-9ad8-df5929ef2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d35741-a09f-4cd6-a73c-57c9dbe847a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94edee28-db28-42a6-8f0e-a99556057ce1",
   "metadata": {},
   "source": [
    "Sort the results by those most probable to be signals since this would likely be the scenario if machine learning is added to EIOS to help analysts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e41de-bdf3-4dc5-9ff9-db79d0e7f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.append(np.array([x_test]).T, y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e7093-c8ca-4ea3-af91-467da9e8562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted[predicted[:,1].argsort()[::-1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
