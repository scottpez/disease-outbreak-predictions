{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fix district name discrepencies between IDSR and geographic boundary files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "There were many districts in the IDSR dataset that did not match the districts by name in the geographic boundaries file. These either came in the form of:\n",
    "1. Names that were originally in a language other than English that were then spelled slightly different when transliterated to English characters.\n",
    "2. Names that were simply different either because it was changed or one of the datasets had incorrect information.\n",
    "\n",
    "To resolve the first one I:\n",
    "1. Used cosine similarity with country, province, and district name to match those with a similar enough name.\n",
    "2. Used the phonetic spelling to match those spelled different but pronounced similar enough.\n",
    "\n",
    "To resolve the second, I manually researched on the Web the remaining districts not matched by the above code and was able to find the matching district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "db_connection_url = \"postgresql://<dbuser>:<dbpasswd>@<dbhost>:<dbport>/<dbname>\"\n",
    "con = create_engine(db_connection_url, isolation_level=\"AUTOCOMMIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soundex(name, len=4):\n",
    "    \"\"\" soundex module conforming to Odell-Russell algorithm \"\"\"\n",
    "\n",
    "    # digits holds the soundex values for the alphabet\n",
    "    soundex_digits = '01230120022455012623010202'\n",
    "    sndx = ''\n",
    "    fc = ''\n",
    "\n",
    "    # Translate letters in name to soundex digits\n",
    "    for c in name.upper(  ):\n",
    "        if c.isalpha(  ):\n",
    "            if not fc: fc = c   # Remember first letter\n",
    "            d = soundex_digits[ord(c)-ord('A')]\n",
    "            # Duplicate consecutive soundex digits are skipped\n",
    "            if not sndx or (d != sndx[-1]):\n",
    "                sndx += d\n",
    "\n",
    "    # Replace first digit with first letter\n",
    "    sndx = fc + sndx[1:]\n",
    "\n",
    "    # Remove all 0s from the soundex code\n",
    "    sndx = sndx.replace('0', '')\n",
    "\n",
    "    # Return soundex code truncated or 0-padded to len characters\n",
    "    return (sndx + (len * '0'))[:len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phonetics\n",
    "import Levenshtein as lev\n",
    "\n",
    "# -- weight\n",
    "weight = {\n",
    "    phonetics.soundex: 0.2,\n",
    "    phonetics.dmetaphone: 0.2,\n",
    "    phonetics.metaphone: 0.5,\n",
    "    phonetics.nysiis: 0.1\n",
    "}\n",
    "\n",
    "# -- algorithms\n",
    "algorithms = [phonetics.soundex, phonetics.dmetaphone, phonetics.metaphone, phonetics.nysiis]\n",
    "    \n",
    "\n",
    "num2words = {1: 'One', 2: 'Two', 3: 'Three', 4: 'Four', 5: 'Five', \\\n",
    "             6: 'Six', 7: 'Seven', 8: 'Eight', 9: 'Nine'}\n",
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "\n",
    "def compare_words_similarity(word1, word2):\n",
    "    # -- total\n",
    "    word1 = word1.replace(\"'\", \"\").replace('(', '').replace(')', '').replace('.', ' ').replace('/', ' ').replace('-', ' ').replace('&', ' ')\n",
    "    word2 = word2.replace(\"'\", \"\").replace('(', '').replace(')', '').replace('.', ' ').replace('/', ' ').replace('-', ' ').replace('&', ' ')\n",
    "    if has_numbers(word1):\n",
    "        word1 = ''.join([' ' + num2words[int(char)] if char.isdigit() else char for char in word1])\n",
    "    if has_numbers(word2):\n",
    "        word2 = ''.join([' ' + num2words[int(char)] if char.isdigit() else char for char in word2])\n",
    "    total = 0.0\n",
    "    for entry in algorithms:\n",
    "        if entry is phonetics.soundex:\n",
    "            c = []\n",
    "            for w in word1.split(' '):\n",
    "                if len(w.strip()) > 0:\n",
    "                    s_len = len(w)-1 if len(w) < 6 else 6\n",
    "                    c.append(entry(w, s_len))\n",
    "            if len(c) == 0:\n",
    "                continue\n",
    "            code1 = ' '.join(c)\n",
    "            c = []\n",
    "            for w in word2.split(' '):\n",
    "                if len(w.strip()) > 0:\n",
    "                    s_len = len(w)-1 if len(w) < 6 else 6\n",
    "                    try:\n",
    "                        c.append(entry(w, s_len))\n",
    "                    except IndexError as ex:\n",
    "                        continue\n",
    "            if len(c) == 0:\n",
    "                continue\n",
    "            code2 = ' '.join(c)\n",
    "        else:\n",
    "            c = []\n",
    "            for w in word1.split(' '):\n",
    "                if len(w.strip()) > 0:\n",
    "                    try:\n",
    "                        c.append(''.join(entry(w)))\n",
    "                    except IndexError as ex:\n",
    "                        continue\n",
    "            if len(c) == 0:\n",
    "                continue\n",
    "            code1 = ' '.join(c)\n",
    "            c = []\n",
    "            for w in word2.split(' '):\n",
    "                if len(w.strip()) > 0:\n",
    "                    try:\n",
    "                        c.append(''.join(entry(w.strip())))\n",
    "                    except IndexError as ex:\n",
    "                        continue\n",
    "            if len(c) == 0:\n",
    "                continue\n",
    "            code2 = ' '.join(c)\n",
    "        lev_score = lev.distance(code1, code2)\n",
    "        currentWeight = weight[entry]\n",
    "        # print (\"comparing %s with %s for %s (%0.2f: weight %0.2f)\" % (code1, code2, entry, lev_score, currentWeight))\n",
    "        subtotal = lev_score * currentWeight\n",
    "        total += subtotal\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_id = 0\n",
    "sql_1 = f'SELECT * FROM district_discrepencies dd WHERE boundaries_adm0_name IS NULL AND id > {starting_id} ORDER BY id ASC LIMIT 2000'\n",
    "df_1 = pd.read_sql_query(sql_1, con)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_2 = 'SELECT * FROM admin2boundaries a2 WHERE '\n",
    "for g in df_1.groupby(['idsr_adm0_name', 'idsr_adm1_name']).groups:\n",
    "    # sql_2 += '(lower(adm0_name) = lower(\\'' + g[0].replace(\"'\", \"''\") + '\\') AND lower(adm1_name) = lower(\\'' + g[1].replace(\"'\", \"''\") + '\\')) OR '\n",
    "    sql_2 += 'lower(adm0_name) = lower(\\'' + g[0].replace(\"'\", \"''\") + '\\') OR '\n",
    "sql_2 = sql_2[:-4]\n",
    "df_2 = pd.read_sql_query(sql_2, con)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['adm0_name_lower'] = df_2['adm0_name'].str.lower()\n",
    "df_2['adm1_name_lower'] = df_2['adm1_name'].str.lower()\n",
    "df_2['adm2_name_lower'] = df_2['adm2_name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function converts non-latin characters to their closest latin character if possible.\n",
    "# For example, 'bahaãƒâ¯ alifa' to 'bahaaa-alifa'.\n",
    "import unicodedata\n",
    "\n",
    "def get_name_slug(name):\n",
    "    formatted_name = name.lower().replace(' ', '-').replace('ı','i')\n",
    "    slug =  unicodedata.normalize('NFD', formatted_name).encode('ascii', 'ignore')\n",
    "\n",
    "    return slug.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_sounds = 8\n",
    "sim_measure = 0.31\n",
    "matches = {}\n",
    "for idx, row in df_1.iterrows():\n",
    "    idsr_adm2_name = row['idsr_adm2_name'].lower()\n",
    "    idsr_adm1_name = row['idsr_adm1_name'].lower()\n",
    "    if not all(x.isalpha() or x.isspace() for x in idsr_adm2_name) and not all(x.isalpha() or x.isspace() for x in idsr_adm1_name):\n",
    "        # print(idsr_adm2name)\n",
    "        continue\n",
    "    \n",
    "    df_3 = df_2[df_2['adm0_name_lower'] == row['idsr_adm0_name'].lower()]\n",
    "    for idx2, row2 in df_3.iterrows():\n",
    "        boundaries_adm2_name = row2['adm2_name_lower']\n",
    "        boundaries_adm1_name = row2['adm1_name_lower']\n",
    "        if not all(x.isalpha() or x.isspace() for x in boundaries_adm2_name) and not all(x.isalpha() or x.isspace() for x in boundaries_adm1_name):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            sim_score_2 = compare_words_similarity(idsr_adm2_name, boundaries_adm2_name)\n",
    "        except IndexError as ex:\n",
    "            sim_score_2 = compare_words_similarity(get_name_slug(idsr_adm2_name), get_name_slug(boundaries_adm2_name))\n",
    "        try:\n",
    "            idsr_sound_2 = ' '.join([soundex(w, number_of_sounds) for w in idsr_adm2_name.split(' ')])\n",
    "        except IndexError as ex:\n",
    "            idsr_sound_2 = ' '.join([soundex(w, number_of_sounds) for w in get_name_slug(idsr_adm2_name).split(' ')])\n",
    "        try:\n",
    "            boundaries_sound_2 = ' '.join([soundex(w, number_of_sounds) for w in boundaries_adm2_name.split(' ')])\n",
    "        except IndexError as ex:\n",
    "            boundaries_sound_2 = ' '.join([soundex(w, number_of_sounds) for w in get_name_slug(boundaries_adm2_name).split(' ')])\n",
    "            \n",
    "        if sim_score_2 < sim_measure or (idsr_sound_2 == boundaries_sound_2 and sim_score_2 < 1):\n",
    "            try:\n",
    "                sim_score_1 = compare_words_similarity(idsr_adm1_name, boundaries_adm1_name)\n",
    "            except IndexError as ex:\n",
    "                sim_score_1 = compare_words_similarity(get_name_slug(idsr_adm1_name), get_name_slug(boundaries_adm1_name))\n",
    "            try:\n",
    "                idsr_sound_1 = ' '.join([soundex(w, number_of_sounds) for w in idsr_adm1_name.split(' ')])\n",
    "            except IndexError as ex:\n",
    "                idsr_sound_1 = ' '.join([soundex(w, number_of_sounds) for w in get_name_slug(idsr_adm1_name).split(' ')])\n",
    "            try:\n",
    "                boundaries_sound_1 = ' '.join([soundex(w, number_of_sounds) for w in boundaries_adm1_name.split(' ')])\n",
    "            except IndexError as ex:\n",
    "                boundaries_sound_1 = ' '.join([soundex(w, number_of_sounds) for w in get_name_slug(boundaries_adm1_name).split(' ')])\n",
    "                \n",
    "            if sim_score_1 < sim_measure or (idsr_sound_1 == boundaries_sound_1 and sim_score_1 < 1):\n",
    "                add_match = (row['idsr_adm0_name'], row['idsr_adm1_name'], row['idsr_adm2_name'], row2['adm0_name'], row2['adm1_name'], row2['adm2_name'], 1)\n",
    "                match_key = f\"{row['idsr_adm0_name']} - {row['idsr_adm1_name']} - {row['idsr_adm2_name']}\"\n",
    "                if match_key in matches:\n",
    "                    # there is already a match for it\n",
    "                    if sim_score_1 < matches[match_key]['sim_score']:\n",
    "                        # this means the second match is a better match so add it.\n",
    "                        matches[match_key] = {\n",
    "                            'sim_score': sim_score_1,\n",
    "                            'match': add_match\n",
    "                        }\n",
    "                else:     \n",
    "                    matches[match_key] = {\n",
    "                        'sim_score': sim_score_1,\n",
    "                        'match': add_match\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_words_similarity(get_name_slug('bahaãƒâ¯ alifa'), get_name_slug('bahaãƒâ¯ bitkine'))\n",
    "get_name_slug('bahaãƒâ¯ alifa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('district_matches.csv', 'w', newline ='', encoding=\"utf-8\") as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(['idsr_adm0name', 'idsr_adm1name', 'idsr_adm2name', 'adm0_name', 'adm1_name', 'adm2_name', 'match'])\n",
    "    write.writerows(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = pd.read_csv('district_matches_done.csv', encoding='utf8')\n",
    "for idx, row in df_4[df_4['match'] == 1].iterrows():\n",
    "    idsr_adm0_name = row[0].replace(\"'\", \"''\")\n",
    "    idsr_adm1_name = row[1].replace(\"'\", \"''\")\n",
    "    idsr_adm2_name = row[2].replace(\"'\", \"''\")\n",
    "    boundaries_adm0_name = row[3].replace(\"'\", \"''\")\n",
    "    boundaries_adm1_name = row[4].replace(\"'\", \"''\")\n",
    "    boundaries_adm2_name = row[5].replace(\"'\", \"''\")\n",
    "    sql_3 = f\"\"\"\n",
    "        UPDATE district_discrepencies dd SET \n",
    "        boundaries_adm0_name='{boundaries_adm0_name}',\n",
    "        boundaries_adm1_name='{boundaries_adm1_name}',\n",
    "        boundaries_adm2_name='{boundaries_adm2_name}' \n",
    "        WHERE \n",
    "        idsr_adm0_name='{idsr_adm0_name}' AND idsr_adm1_name='{idsr_adm1_name}' AND idsr_adm2_name='{idsr_adm2_name}';\n",
    "    \"\"\"\n",
    "    # print(sql_3)\n",
    "    con.execute(sql_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process supplemental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the datasets below follow a similar processing workflow with only small variations:\n",
    "1. Write a txt file that contains all URLS to download the data.\n",
    "2. Run wget with the txt file to download files.\n",
    "3. Run the ZonalStatisticsAsTable ArcGIS Pro tool to get summary values by district. The statistics is either the MEAN or SUM of cell values depending on what makes snese for the data.\n",
    "4. Import the summary statistics into the DB and query to find any missing districts that were missed because they are too small or on the edge of the data and no cell is completely within it.\n",
    "5. Find values for the missing districts by getting the value of the nearest cell.\n",
    "6. Import the missing values into the same DB table as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download precititation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime.strptime('2019-01-01', \"%Y-%m-%d\") - timedelta(days=7)\n",
    "end_date = datetime.strptime('2022-12-31', \"%Y-%m-%d\")\n",
    "download_path = '/path-to-data/precipitation/'\n",
    "user = 'scottpez'\n",
    "passwd = '5c4Nad%CyX'\n",
    "basic = HTTPBasicAuth(user, passwd)\n",
    "urls = []\n",
    "\n",
    "for dt in rrule.rrule(rrule.DAILY, dtstart=start_date, until=end_date):\n",
    "    y = datetime.strftime(dt, '%Y')\n",
    "    m = datetime.strftime(dt, '%m')\n",
    "    d = datetime.strftime(dt, '%d')\n",
    "    file_name = f'3B-DAY-E.MS.MRG.3IMERG.{y}{m}{d}-S000000-E235959.V06.nc4'\n",
    "    url = f'https://gpm1.gesdisc.eosdis.nasa.gov/data/GPM_L3/GPM_3IMERGDE.06/{y}/{m}/{file_name}'\n",
    "    urls.append(url)\n",
    "\n",
    "with open(download_path + 'urls.txt', 'w') as f:\n",
    "    for url in urls:\n",
    "        f.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command below in a comand prompt to download the files from the list of URLs in urls.txt produced above.\n",
    "```\n",
    "wget --content-disposition --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies --content-disposition --user=scottpez --password=5c4Nad%CyX -i urls.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process precipitation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil import rrule\n",
    "from arcpy.sa import Raster\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "start_date = datetime.strptime('2019-01-01', \"%Y-%m-%d\") - timedelta(days=7)\n",
    "end_date = datetime.strptime('2022-11-30', \"%Y-%m-%d\")\n",
    "arcpy.env.workspace = '/path-to-data/precipitation/'\n",
    "\n",
    "for dt in rrule.rrule(rrule.DAILY, dtstart=start_date, until=end_date):\n",
    "    y = datetime.strftime(dt, '%Y')\n",
    "    m = datetime.strftime(dt, '%m')\n",
    "    d = datetime.strftime(dt, '%d')\n",
    "    file_name = f'3B-DAY-E.MS.MRG.3IMERG.{y}{m}{d}-S000000-E235959.V06.nc4'\n",
    "    tif_name = file_name.replace('.nc4', '.tif')\n",
    "    if arcpy.Exists(file_name) and not arcpy.Exists(tif_name):\n",
    "        arcpy.md.MakeNetCDFRasterLayer(file_name, \"HQprecipitation\", \"lon\", \"lat\", \"precipitation\", \"time\", None, \"BY_VALUE\", \"CENTER\")\n",
    "        r = Raster('precipitation')\n",
    "        r.save(tif_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the HQprecipitation (high-quality precipitation) channel from the nc4 file and save it as a tif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr \n",
    "import rioxarray as rio\n",
    "from dask.utils import SerializableLock\n",
    "\n",
    "lock = SerializableLock()\n",
    "\n",
    "path = '/path-to-data/precipitation/'\n",
    "for f in os.listdir(path):\n",
    "    if f.endswith('.nc4') and not os.path.exists(path + os.sep + f.replace('.nc4', '.tif')):\n",
    "        print(f'doing {f}')\n",
    "        nc_file = xr.open_dataset(path + os.sep + f)\n",
    "        p = nc_file[\"HQprecipitation\"]\n",
    "        p = p.rio.set_spatial_dims(x_dim='lon', y_dim='lat')\n",
    "        p.rio.crs\n",
    "        p.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "        p = p.transpose('time', 'lat', 'lon')\n",
    "        p = p.rio.set_spatial_dims(x_dim='lon', y_dim='lat')\n",
    "        p.rio.to_raster(path + os.sep + f.replace('.nc4', '.tif'), lock=lock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using arcpy, find the statistics for precipitation within every administrative district and save as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import arcpy\n",
    "from arcpy.ia import *\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "idsr_weeks_file = '/path-to-data/20230625_idsr_weeks.csv' # this file simply contains a list exported from IDSR with each week on one line.\n",
    "df_5 = pd.read_csv(idsr_weeks_file, encoding='utf8')\n",
    "\n",
    "def group_rasters(path, dir_name, stat):\n",
    "    \n",
    "    arcpy.env.workspace = path + dir_name + '/'\n",
    "    \n",
    "    groups = {}\n",
    "    for i, row in df_5.iterrows():\n",
    "        d2 = pd.to_datetime(row['epidemic_week'])\n",
    "        d1 = d2 - timedelta(days=7)\n",
    "        grp = []\n",
    "        while d1 < d2:\n",
    "            d_format = d1.strftime('%Y%m%d')\n",
    "            if dir_name == 'precipitation':\n",
    "                raster_name = f'3B-DAY-E.MS.MRG.3IMERG.{d_format}-S000000-E235959.V06.tif'\n",
    "            elif dir_name == 'temperature':\n",
    "                raster_name = f'GLDAS_NOAH025_3H.A{d_format}.1500.021.tif'\n",
    "            grp.append(raster_name)\n",
    "            d1 = d1 + timedelta(days=1)\n",
    "        groups[d2.strftime('%Y%m%d')] = grp\n",
    "    \n",
    "    for k in groups.keys():\n",
    "        if dir_name == 'precipitation':\n",
    "            raster_name = f'3B-DAY-E.MS.MRG.3IMERG.{k}-S000000-E235959.V06_week.tif'\n",
    "        elif dir_name == 'temperature':\n",
    "            raster_name = f'GLDAS_NOAH025_3H.A{k}.1500.021_week.tif'\n",
    "        if not arcpy.Exists(raster_name):\n",
    "            print(f'doing {k}')\n",
    "            rc = RasterCollection(groups[k])\n",
    "            if stat == 'SUM':\n",
    "                range_raster = Sum(rc)\n",
    "            elif stat == 'MEAN':\n",
    "                range_raster = Mean(rc)\n",
    "            range_raster.save(raster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_rasters('/path-to-data/', 'precipitation', 'MEAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import arcpy\n",
    "\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "\n",
    "def find_zonal_statistics_for_districts(arc_workspace, stat):\n",
    "    arcpy.env.workspace = arc_workspace\n",
    "\n",
    "    admin_shp = '/path-to-data/Admin2_Afro_Master_final.shp'\n",
    "    gdb_path = '/path-to-geodatabase/WHO.gdb'  # I used a geodatabase for this because when I used TIF files it seemed to crash often.\n",
    "\n",
    "    rasters = arcpy.ListRasters(\"*\", \"TIF\")\n",
    "    for raster in rasters:\n",
    "        out_csv = raster.replace('.', '_').replace('_tif', 'test.csv')\n",
    "        if '_week' in raster and not os.path.exists(f'{arcpy.env.workspace}/{out_csv}'):\n",
    "            print(f'doing {raster}')\n",
    "            out_table = 'temp'\n",
    "            with arcpy.EnvManager(cellSize=0.125):\n",
    "                arcpy.ia.ZonalStatisticsAsTable(\n",
    "                    in_zone_data=admin_shp,\n",
    "                    zone_field=\"MY_ID\",\n",
    "                    in_value_raster=raster,\n",
    "                    out_table=f'{gdb_path}/{out_table}',\n",
    "                    ignore_nodata=\"DATA\",\n",
    "                    statistics_type=stat,\n",
    "                    process_as_multidimensional=\"CURRENT_SLICE\",\n",
    "                    percentile_values=[90],\n",
    "                    percentile_interpolation_type=\"AUTO_DETECT\",\n",
    "                    circular_calculation=\"ARITHMETIC\",\n",
    "                    circular_wrap_value=360\n",
    "                )\n",
    "            arcpy.conversion.ExportTable(f'{gdb_path}/{out_table}', f'{arcpy.env.workspace}/{out_csv}')\n",
    "\n",
    "            arcpy.management.Delete(f'{gdb_path}/{out_table}')\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_zonal_statistics_for_districts('/path-to-data/precipitation/', 'MEAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the zonal statistics tool above will miss polygons if those polygons are very small and the boundary does not overlap the center of any raster grid cell. To solve this issue, I will perform the steps below after finding the adm2_code for the polygons that were missed. Basically, it converts the raster to points and finds the nearest point to the missing polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import arcpy\n",
    "\n",
    "\n",
    "def find_zonal_statistics_for_districts_missing(arc_workspace, stat, missing_file_name):\n",
    "    arcpy.env.workspace = arc_workspace\n",
    "    arcpy.env.overwriteOutput = True\n",
    "\n",
    "    admin_shp = '/path-to-data/Admin2_Afro_Master_final.shp'\n",
    "    gdb_path = '/path-to-geodatabase/WHO.gdb'\n",
    "    temp_points_layer = 'temp_points'\n",
    "\n",
    "    myids_not_matched = []\n",
    "    with open(f'{arcpy.env.workspace}/{missing_file_name}', newline='') as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        headers = next(csvreader, None)\n",
    "        for row in csvreader:\n",
    "            myids_not_matched.append(row[0])\n",
    "\n",
    "    tmp_clause = ','.join(myids_not_matched)\n",
    "    tmp_clause = tmp_clause.replace('\"', '\\'')\n",
    "    where_clause = f\"Admin2_Afro_Master_final.MY_ID IN ({tmp_clause})\"\n",
    "    sel_features = arcpy.management.SelectLayerByAttribute(\n",
    "        in_layer_or_view=admin_shp,\n",
    "        selection_type=\"NEW_SELECTION\",\n",
    "        where_clause=where_clause,\n",
    "        invert_where_clause=None\n",
    "    )\n",
    "    ext = arcpy.Describe(sel_features).extent\n",
    "    buf = 0.1\n",
    "    ext = arcpy.Extent(ext.XMin-buf, ext.YMin-buf, ext.XMax+buf, ext.XMax+buf)\n",
    "    with arcpy.EnvManager(extent=ext):\n",
    "        rasters = arcpy.ListRasters(\"*\", \"TIF\")\n",
    "        for raster in rasters:\n",
    "            out_csv = raster.replace('.', '_').replace('_tif', '_2.csv')\n",
    "            if '_week' in raster and not os.path.exists(f'{arcpy.env.workspace}/{out_csv}'):\n",
    "                print(f'doing {raster}')\n",
    "                arcpy.conversion.RasterToPoint(\n",
    "                    in_raster=raster,\n",
    "                    out_point_features=f\"{gdb_path}/{temp_points_layer}\",\n",
    "                    raster_field=\"Value\"\n",
    "                )\n",
    "                result = arcpy.management.AddSpatialJoin(\n",
    "                    target_features=sel_features,\n",
    "                    join_features=f\"{gdb_path}/{temp_points_layer}\",\n",
    "                    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "                    join_type=\"KEEP_ALL\",\n",
    "                    field_mapping=f'pointid \"pointid\" true true false 4 Long 0 0,First,#,{gdb_path}/{temp_points_layer},pointid,-1,-1;grid_code \"grid_code\" true true false 4 Float 0 0,First,#,{gdb_path}/{temp_points_layer},grid_code,-1,-1',\n",
    "                    match_option=\"CLOSEST_GEODESIC\",\n",
    "                    search_radius=0.1,\n",
    "                    distance_field_name=\"\"\n",
    "                )\n",
    "                fields = arcpy.ListFields(result)\n",
    "                find_fields = []\n",
    "                for field in fields:\n",
    "                    if 'MY_ID' in field.name or 'grid_code' in field.name:\n",
    "                        find_fields.append(field.name)\n",
    "\n",
    "                result_rows = []\n",
    "                with arcpy.da.SearchCursor(result, find_fields) as cursor:\n",
    "                    for row in cursor:\n",
    "                        result_rows.append([row[0], 1, row[1]])\n",
    "\n",
    "                with open(f'{arcpy.env.workspace}/{out_csv}', 'w', newline='') as outcsv:\n",
    "                    writer = csv.writer(outcsv)\n",
    "                    writer.writerow([\"MY_ID\", \"COUNT\", stat])\n",
    "                    for r in result_rows:\n",
    "                        writer.writerow(r)\n",
    "\n",
    "                arcpy.management.Delete(f\"{gdb_path}/{temp_points_layer}\")\n",
    "        arcpy.management.Delete(f\"{gdb_path}/sel_features_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in all precipitation CSV files and aggregate the daily precipitation to the epidemic weeks and divide by the  to find the average temperature in the district for each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def group_statistics_by_district_time(dir_name, stat, step=1, clas=None):\n",
    "    path = f'/path-to-data/{dir_name}/'\n",
    "    dfs = []\n",
    "    for f in os.listdir(path):\n",
    "        if (dir_name == 'temperature' and '_week' in f and f.endswith('.csv') and f.startswith('GLDAS')) \\\n",
    "        or (dir_name == 'precipitation' and '_week' in f and f.endswith('.csv') and f.startswith('3B-DAY')) \\\n",
    "        or (dir_name == 'landcover' and f.endswith(f'{clas}_week.csv')):\n",
    "            if (step == 1 and '_2.csv' not in f) or step == 2:\n",
    "                if '_2.csv' in f:\n",
    "                    df = pd.read_csv(path + f, usecols = ['MY_ID', 'COUNT', 'SUM'])\n",
    "                    df.rename(columns = {'SUM':'MEAN'}, inplace = True)\n",
    "                else:\n",
    "                    df = pd.read_csv(path + f, usecols = ['MY_ID', 'COUNT', stat])\n",
    "                # df = pd.read_csv(path + f, usecols = ['MY_ID', 'COUNT', stat])\n",
    "                if dir_name == 'temperature':\n",
    "                    date_start_idx = 18\n",
    "                elif dir_name == 'precipitation':\n",
    "                    date_start_idx = 23\n",
    "                dd = datetime.strptime(f[date_start_idx:date_start_idx+8], '%Y%m%d')\n",
    "                df['date'] = dd\n",
    "                dfs.append(df)\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "    \n",
    "    with open(f'{path}{dir_name}_full_{step}.csv', 'w', newline='') as csvfile:\n",
    "        fieldnames = ['my_id', 'epidemic_week', f'val_{dir_name}']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for idx, row in df.iterrows():\n",
    "            val = row[stat]\n",
    "            if dir_name == 'temperature':\n",
    "                val = val - 273.15  # convert kelvin temperature to celsius\n",
    "            writer.writerow({'my_id': row['MY_ID'], 'epidemic_week': row['date'], f'val_{dir_name}': val})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the zonal statsitics tool above will miss polygons if those polygons are very small and the boundary does not overlap the center of any raster grid cell. To solve this issue, I will perform the steps below after finding the adm2_code for the polygons that were missed. Basically, it converts the raster to points and finds the nearest point to the missing polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_statistics_by_district_time('precipitation', 'MEAN', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Above, I grouped the statistics for precipitation by adm2 and week.\n",
    "2. Next, import this CSV file into the database and run the query to see which districts were not matched to the precipitation grid either because they are too small or right on an edge.\n",
    "3. Find all districts not matched and save their adm2_code's to a CSV file to be read in below.\n",
    "4. Once all districts are matched, then reimport the precipitation CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_zonal_statistics_for_districts_missing('/path-to-data/precipitation/', 'MEAN', '20230626_districtsWithNoPrecipitationMatch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_statistics_by_district_time('precipitation', 'MEAN', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify precipitation raster and clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, I was previously simplifying the rasters because I was running the zonal statistics in PostGIS. It was much faster in ArcGIS Pro so I didn't have to do this anymore. So, I am not using the code in the cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil import rrule\n",
    "from arcpy.sa import Raster\n",
    "from arcpy.sa import Aggregate\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.addOutputsToMap = False\n",
    "start_date = datetime.strptime('2019-01-01', \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime('2022-11-30', \"%Y-%m-%d\")\n",
    "arcpy.env.workspace = 'D:\\\\work\\\\BrightWorldLabs\\\\consulting\\\\2023\\\\WHO-AFRO\\\\data\\\\precipitation\\\\'\n",
    "\n",
    "for dt in rrule.rrule(rrule.DAILY, dtstart=start_date, until=end_date):\n",
    "    y = datetime.strftime(dt, '%Y')\n",
    "    m = datetime.strftime(dt, '%m')\n",
    "    d = datetime.strftime(dt, '%d')\n",
    "    file_name = f'3B-DAY-E.MS.MRG.3IMERG.{y}{m}{d}-S000000-E235959.V06.tif'\n",
    "    print(file_name)\n",
    "    new_tif_name = file_name.replace('.tif', '_rc.tif')\n",
    "    OutRas = Aggregate(file_name, 4)\n",
    "    arcpy.management.Clip(OutRas, \n",
    "                          \"-16.4316406249999 -34.7528343199999 49.321220398 37.2018585210001\", \n",
    "                          new_tif_name, \n",
    "                          r\"D:\\work\\BrightWorldLabs\\consulting\\2023\\WHO-AFRO\\data\\africa_hull.shp\", \n",
    "                          \"-9999.9\", \n",
    "                          \"ClippingGeometry\",\n",
    "                          \"NO_MAINTAIN_EXTENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the temperature data\n",
    "\n",
    "Below it will create a txt file that contains the URLs for all iamges to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime.strptime('2019-01-01', \"%Y-%m-%d\") - timedelta(days=7)\n",
    "end_date = datetime.strptime('2022-02-17', \"%Y-%m-%d\")\n",
    "download_path = '/path-to-data/temperature/'\n",
    "user = 'scottpez'\n",
    "passwd = '5c4Nad%CyX'\n",
    "basic = HTTPBasicAuth(user, passwd)\n",
    "urls = []\n",
    "\n",
    "for dt in rrule.rrule(rrule.DAILY, dtstart=start_date, until=end_date):\n",
    "    y = datetime.strftime(dt, '%Y')\n",
    "    m = datetime.strftime(dt, '%m')\n",
    "    d = datetime.strftime(dt, '%d')\n",
    "    w = datetime.strftime(dt, '%j')\n",
    "    h = '15'\n",
    "    mi = '00'\n",
    "    \n",
    "    file_name = f'GLDAS_NOAH025_3H.A{y}{m}{d}.{h}{mi}.021.nc4'\n",
    "    url = f'https://data.gesdisc.earthdata.nasa.gov/data/GLDAS/GLDAS_NOAH025_3H.2.1/{y}/{w}/{file_name}'\n",
    "    urls.append(url)\n",
    "\n",
    "with open(download_path + 'urls.txt', 'w') as f:\n",
    "    for url in urls:\n",
    "        f.write(f'{url}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, run the wget command below to download all image files listed in the text file created above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command below in a command line.\n",
    "\n",
    "wget --content-disposition --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies --content-disposition --user=scottpez --password=5c4Nad%CyX -i urls.txt\n",
    "\n",
    "The above command will save the file name with a lengthy extension that adds the parameters. So, next, create a bash shell script in the same folder, save it, and run it.\n",
    "\n",
    "```\n",
    "#!/bin/sh\n",
    "\n",
    "for f in *\n",
    "  do\n",
    "  name=$(echo $f| cut -c 1-39)\n",
    "  command=$(mv $f $name)\n",
    "  $command\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the NC4 file, extract the layer for temperature to its own TIF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr \n",
    "import rioxarray as rio\n",
    "from dask.utils import SerializableLock\n",
    "\n",
    "lock = SerializableLock()\n",
    "\n",
    "path = '/path-to-data/temperature/'\n",
    "for f in os.listdir(path):\n",
    "    tif_path = path + f.replace('.nc4', '.tif')\n",
    "    if f.endswith('.nc4') and not os.path.exists(tif_path):\n",
    "        print(f)\n",
    "        nc_file = xr.open_dataset(path + os.sep + f)\n",
    "        p = nc_file[\"Tair_f_inst\"]\n",
    "        p = p.rio.set_spatial_dims(x_dim='lon', y_dim='lat')\n",
    "        p.rio.crs\n",
    "        p.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "        p = p.transpose('time', 'lat', 'lon')\n",
    "        p = p.rio.set_spatial_dims(x_dim='lon', y_dim='lat')\n",
    "        p.rio.to_raster(path + os.sep + f.replace('.nc4', '.tif'), lock=lock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "group_rasters('/path-to-data/', 'temperature', 'MEAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using arcpy, find the statistics for temperature within every administrative district and save as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_zonal_statistics_for_districts('/path-to-data/temperature/', 'MEAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the zonal statsitics tool above will miss polygons if those polygons are very small and the boundary does not overlap the center of any raster grid cell. To solve this issue, I will perform the steps below after finding the adm2_code for the polygons that were missed. Basically, it converts the raster to points and finds the nearest point to the missing polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_statistics_by_district_time('temperature', 'MEAN', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Above, I grouped the statistics for precipitation by adm2 and week.\n",
    "2. Next, import this CSV file into the database and run the query to see which districts were not matched to the precipitation grid either because they are too small or right on an edge.\n",
    "3. Find all districts not matched and save their adm2_code's to a CSV file to be read in below.\n",
    "4. Once all districts are matched, then reimport the precipitation CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_zonal_statistics_for_districts_missing('/path-to-data/temperature/', 'MEAN', '20230627_districtsWithNoTemperatureMatch.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in all temperature CSV files and aggregate the daily temperature to the epidemic weeks to find the average temperature in the district for each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_statistics_by_district_time('temperature', 'MEAN', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download landcover data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hosted by Microsoft.\n",
    "\n",
    "https://planetarycomputer.microsoft.com/dataset/io-lulc-9-class\n",
    "\n",
    "Produced by Impact Observatory using deep learning model on Copernicus Sentinel 2 imagery.\n",
    "\n",
    "https://www.impactobservatory.com/global_maps/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "download_path = '/path-to-data/landcover/'\n",
    "rows = ['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S']\n",
    "cols = [26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41]\n",
    "years = [2018, 2020, 2021, 2022]\n",
    "\n",
    "for year in years:\n",
    "    if not os.path.exists(download_path + str(year)):\n",
    "        os.mkdir(download_path + str(year))\n",
    "    for col in cols:\n",
    "        for row in rows:\n",
    "            file_name = f'{col}{row}_{year}0101-{year+1}0101.tif'\n",
    "            \n",
    "            if not os.path.exists(download_path + f'{year}/{file_name}'):\n",
    "                print('Downloading: ',file_name)\n",
    "                url = f'https://lulctimeseries.blob.core.windows.net/lulctimeseriespublic/lc{year}/{col}{row}_{year}0101-{year+1}0101.tif'\n",
    "                r = requests.get(url, stream=True)\n",
    "\n",
    "                # Check if the image was retrieved successfully\n",
    "                if r.status_code == 200:\n",
    "                    # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "                    r.raw.decode_content = True\n",
    "\n",
    "                    # Open a local file with wb ( write binary ) permission.\n",
    "                    with open(download_path + f'{year}/{file_name}', 'wb') as f:\n",
    "                        shutil.copyfileobj(r.raw, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process landcover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mosaic for landcover, clip it to the boundary of Africa, and save it as a tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import arcpy\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.addOutputsToMap = False\n",
    "gdb_path = '/path-to-geodatabase/WHO.gdb'\n",
    "\n",
    "years = [2018, 2019, 2020, 2021, 2022]\n",
    "\n",
    "for year in years:\n",
    "    print(f'doing {year}')\n",
    "    arcpy.env.workspace = '/path-to-data/landcover' + os.sep + str(year) + os.sep\n",
    "    raster_names = []\n",
    "    rasters = arcpy.ListRasters(\"*\", \"TIF\")\n",
    "    for raster in rasters:\n",
    "        new_raster = raster.replace('.tif', '_4326.tif')\n",
    "        if not '4326' in raster and not arcpy.Exists(new_raster):\n",
    "            print(f'doing {raster}')\n",
    "            arcpy.management.ProjectRaster(\n",
    "                in_raster=raster,\n",
    "                out_raster=new_raster,\n",
    "                out_coor_system='GEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137.0,298.257223563]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]]'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import arcpy\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.addOutputsToMap = False\n",
    "gdb_path = '/path-to-data/WHO.gdb'\n",
    "\n",
    "years = [2018, 2019, 2020, 2021, 2022]\n",
    "\n",
    "for year in years:\n",
    "    print(f'doing {year}')\n",
    "    arcpy.env.workspace = '/path-to-data/landcover' + os.sep + str(year) + os.sep\n",
    "    raster_names = []\n",
    "    rasters = arcpy.ListRasters(\"*\", \"TIF\")\n",
    "    for raster in rasters:\n",
    "        if '4326' in raster:\n",
    "            raster_names.append(raster)\n",
    "    all_rasters = ';'.join(raster_names)\n",
    "    arcpy.management.CreateRasterDataset(gdb_path, f\"lc_{year}\", \"0.00736\", \"8_BIT_UNSIGNED\", None)\n",
    "    arcpy.management.Mosaic(\n",
    "        inputs=all_rasters,\n",
    "        target=gdb_path + os.sep + f\"lc_{year}\",\n",
    "        mosaic_type=\"LAST\",\n",
    "        colormap=\"FIRST\",\n",
    "        background_value=None,\n",
    "        nodata_value=None,\n",
    "        onebit_to_eightbit=\"NONE\",\n",
    "        mosaicking_tolerance=0,\n",
    "        MatchingMethod=\"NONE\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the values for different land covers and do zonal statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.addOutputsToMap = False\n",
    "\n",
    "years = [2018, 2019, 2020, 2021, 2022]\n",
    "clas = {\n",
    "    'trees': 2, # this is the class value for trees\n",
    "    'crops': 5,\n",
    "    'builtup': 7,\n",
    "    'bareground': 8,\n",
    "    'rangeland': 11\n",
    "}\n",
    "out_table = 'temp'\n",
    "gdb_path = '/path-to-geodatabase/WHO.gdb'\n",
    "arcpy.env.workspace = gdb_path\n",
    "admin_shp = '/path-to-data/Admin2_Afro_Master_final.shp'\n",
    "\n",
    "for year in years:\n",
    "    raster = f\"lc_{year}\"\n",
    "    for k, v in clas.items():\n",
    "        print(f'doing {raster}_{k}')\n",
    "        out_csv = f'{raster}_{k}.csv'\n",
    "        sub_ras = arcpy.ia.Con(\n",
    "            in_conditional_raster=raster,\n",
    "            in_true_raster_or_constant=1,\n",
    "            in_false_raster_or_constant=0,\n",
    "            where_clause=f\"VALUE = {v}\"\n",
    "        ) # if it is the class then set it equal to 1, if not 0.\n",
    "        arcpy.ia.ZonalStatisticsAsTable(\n",
    "            in_zone_data=admin_shp,\n",
    "            zone_field=\"MY_ID\",\n",
    "            in_value_raster=sub_ras,\n",
    "            out_table=f'{gdb_path}/{out_table}',\n",
    "            ignore_nodata=\"DATA\",\n",
    "            statistics_type='SUM'\n",
    "        )\n",
    "        arcpy.conversion.ExportTable(f'{gdb_path}/{out_table}', f'/path-to-data/landcover/{year}/{out_csv}')\n",
    "        arcpy.management.Delete(f'{gdb_path}/{out_table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "\n",
    "\n",
    "arcpy.env.workspace = '/path-to-geodatabase/WHO.gdb'\n",
    "years = [2018, 2019, 2020, 2021, 2022]\n",
    "\n",
    "for year in years:\n",
    "    raster = f\"lc_{year}\"\n",
    "    arcpy.management.CalculateStatistics(raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import arcpy\n",
    "\n",
    "\n",
    "years = [2018, 2019, 2020, 2021, 2022]\n",
    "clas = {\n",
    "    'trees': 2, # this is the class value for trees\n",
    "    'crops': 5,\n",
    "    'builtup': 7,\n",
    "    'bareground': 8,\n",
    "    'rangeland': 11\n",
    "}\n",
    "arcpy.env.workspace = '/path-to-geodatabase/WHO.gdb'\n",
    "arcpy.env.overwriteOutput = True\n",
    "stat = 'SUM'\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    raster = f\"lc_{year}\"\n",
    "    admin_shp = '/path-to-data/Admin2_Afro_Master_final.shp'\n",
    "    gdb_path = '/path-to-geodatabase/WHO.gdb'\n",
    "    temp_points_layer = 'temp_points'\n",
    "    missing_file_name = '/path-to-data/20230630_districtsWithNoLandcoverMatch.csv'\n",
    "\n",
    "    myids_not_matched = []\n",
    "    with open(missing_file_name, newline='') as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        headers = next(csvreader, None)\n",
    "        for row in csvreader:\n",
    "            myids_not_matched.append(row[0])\n",
    "\n",
    "    tmp_clause = ','.join(myids_not_matched)\n",
    "    tmp_clause = tmp_clause.replace('\"', '\\'')\n",
    "    where_clause = f\"Admin2_Afro_Master_final.MY_ID IN ({tmp_clause})\"\n",
    "    sel_features = arcpy.management.SelectLayerByAttribute(\n",
    "        in_layer_or_view=admin_shp,\n",
    "        selection_type=\"NEW_SELECTION\",\n",
    "        where_clause=where_clause,\n",
    "        invert_where_clause=None\n",
    "    )\n",
    "    ext = arcpy.Describe(sel_features).extent\n",
    "    buf = 0.1\n",
    "    ext = arcpy.Extent(ext.XMin-buf, ext.YMin-buf, ext.XMax+buf, ext.XMax+buf)\n",
    "    with arcpy.EnvManager(extent=ext):\n",
    "        out_csv = f'{raster}_2.csv'\n",
    "        if not os.path.exists(f'/path-to-data/landcover/{year}/{out_csv}'):\n",
    "            print(f'doing {raster}')\n",
    "            arcpy.conversion.RasterToPoint(\n",
    "                in_raster=raster,\n",
    "                out_point_features=f\"{gdb_path}/{temp_points_layer}\",\n",
    "                raster_field=\"Value\"\n",
    "            )\n",
    "            result = arcpy.management.AddSpatialJoin(\n",
    "                target_features=sel_features,\n",
    "                join_features=f\"{gdb_path}/{temp_points_layer}\",\n",
    "                join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "                join_type=\"KEEP_ALL\",\n",
    "                field_mapping=f'pointid \"pointid\" true true false 4 Long 0 0,First,#,{gdb_path}/{temp_points_layer},pointid,-1,-1;grid_code \"grid_code\" true true false 4 Float 0 0,First,#,{gdb_path}/{temp_points_layer},grid_code,-1,-1',\n",
    "                match_option=\"CLOSEST_GEODESIC\",\n",
    "                search_radius=0.1,\n",
    "                distance_field_name=\"\"\n",
    "            )\n",
    "            fields = arcpy.ListFields(result)\n",
    "            find_fields = []\n",
    "            for field in fields:\n",
    "                if 'MY_ID' in field.name or 'grid_code' in field.name:\n",
    "                    find_fields.append(field.name)\n",
    "\n",
    "            result_rows = []\n",
    "            with arcpy.da.SearchCursor(result, find_fields) as cursor:\n",
    "                for row in cursor:\n",
    "                    result_rows.append([row[0], 1, row[1]])\n",
    "\n",
    "            with open(f'/path-to-data/landcover/{year}/{out_csv}', 'w', newline='') as outcsv:\n",
    "                writer = csv.writer(outcsv)\n",
    "                writer.writerow([\"MY_ID\", \"COUNT\", \"CLAS\"])\n",
    "                for r in result_rows:\n",
    "                    writer.writerow(r)\n",
    "\n",
    "            arcpy.management.Delete(f\"{gdb_path}/{temp_points_layer}\")\n",
    "    arcpy.management.Delete(f\"{gdb_path}/sel_features_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "years = [2018, 2019, 2020, 2021, 2022]\n",
    "clas = {\n",
    "    'trees': 2, # this is the class value for trees\n",
    "    'crops': 5,\n",
    "    'builtup': 7,\n",
    "    'bareground': 8,\n",
    "    'rangeland': 11\n",
    "}\n",
    "\n",
    "\n",
    "def group_landcover(dir_name, stat, step=1, clas=None):\n",
    "    path = f'/path-to-data/{dir_name}/'\n",
    "    dfs = []\n",
    "    for y in years:\n",
    "        f = f'{path}{y}/lc_{y}_2.csv'\n",
    "        df_1 = pd.read_csv(f, usecols = ['MY_ID', 'COUNT', 'CLAS'])\n",
    "        lookup = {}\n",
    "        for idx, row in df_1.iterrows():\n",
    "            my_id = row[0]\n",
    "            my_clas = row[2]\n",
    "            for k, v in clas.items():\n",
    "                if k not in lookup:\n",
    "                    lookup[k] = []\n",
    "                if v == my_clas:\n",
    "                    lookup[k].append([my_id, 1])\n",
    "                else:\n",
    "                    lookup[k].append([my_id, 0])\n",
    "                \n",
    "        for k in clas.keys():\n",
    "            f = f'{path}{y}/lc_{y}_{k}.csv'\n",
    "            df = pd.read_csv(f, usecols = ['MY_ID', 'COUNT', stat])\n",
    "            new_data = []\n",
    "            for r in lookup[k]:\n",
    "                new_data.append([r[0], 1, r[1]])\n",
    "            new_df = pd.DataFrame(new_data, columns=['MY_ID', 'COUNT', stat])\n",
    "            df = pd.concat([df, new_df])\n",
    "            df['year'] = y\n",
    "            df['clas'] = k\n",
    "            dfs.append(df)\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "    df['PERCENT_COV'] = df[stat] / df['COUNT']\n",
    "    \n",
    "    with open(f'{path}{dir_name}_full_{step}.csv', 'w', newline='') as csvfile:\n",
    "        fieldnames = ['my_id', 'year', 'clas', 'val', 'percent_cov']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for i, row in df.iterrows():\n",
    "            writer.writerow({'my_id': row['MY_ID'], 'year': row['year'], 'clas': row['clas'], 'val': row[stat], 'percent_cov': row['PERCENT_COV']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_landcover('landcover', 'SUM', 2, clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create data that shows the population and population near water bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from arcpy.sa import *\n",
    "\n",
    "admin_shp = '/path-to-data/Admin2_Afro_Master_subset1.shp'\n",
    "gdb_path = '/path-to-geodatabase/WHO.gdb'\n",
    "waterbodies_buffer = '/path-to-data/osm_waterbodies_3km.shp'\n",
    "new_waterbodies_buffer = waterbodies_buffer.split('\\\\')[-1].replace('.shp', '.tif')\n",
    "arcpy.conversion.PolygonToRaster(\n",
    "    in_features=waterbodies_buffer,\n",
    "    value_field=\"FID\",\n",
    "    out_rasterdataset=gdb_path + os.sep + new_waterbodies_buffer,\n",
    "    cell_assignment=\"CELL_CENTER\",\n",
    "    priority_field=\"NONE\",\n",
    "    cellsize=\"/path-to-data/population/ppp_2020_1km_Aggregated.tif\",\n",
    "    build_rat=\"BUILD\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from arcpy.sa import *\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "new_waterbodies_buffer = '/path-to-geodatabase/WHO.gdb/osm_waterbodies_3km'\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "for y in years:\n",
    "    print(f'doing {y}')\n",
    "    population_raster = f'/path-to-data/population/ppp_{y}_1km_Aggregated.tif'\n",
    "    raster = Raster(new_waterbodies_buffer)\n",
    "    con_temp = Con(in_conditional_raster=raster, in_true_raster_or_constant=population_raster, in_false_raster_or_constant=0, where_clause=\"VALUE > 0\")\n",
    "    con_raster = Con(\n",
    "        in_conditional_raster=con_temp,\n",
    "        in_true_raster_or_constant=0,\n",
    "        in_false_raster_or_constant=con_temp,\n",
    "        where_clause=\"VALUE IS NULL\"\n",
    "    )\n",
    "    con_raster.save(population_raster.replace('.tif', '_water.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.addOutputsToMap = False\n",
    "\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "out_table = 'temp'\n",
    "gdb_path = '/path-to-geodatabase/WHO.gdb'\n",
    "arcpy.env.workspace = gdb_path\n",
    "admin_shp = '/path-to-data/Admin2_Afro_Master_final.shp'\n",
    "\n",
    "for y in years:\n",
    "    raster = f'/path-to-data/population/ppp_{y}_1km_Aggregated_water.tif'\n",
    "    print(f'doing {raster}')\n",
    "    out_csv = raster.replace('.tif', '.csv')\n",
    "    arcpy.ia.ZonalStatisticsAsTable(\n",
    "        in_zone_data=admin_shp,\n",
    "        zone_field=\"MY_ID\",\n",
    "        in_value_raster=raster,\n",
    "        out_table=f'{gdb_path}/{out_table}',\n",
    "        ignore_nodata=\"DATA\",\n",
    "        statistics_type='SUM'\n",
    "    )\n",
    "    arcpy.conversion.ExportTable(f'{gdb_path}/{out_table}', out_csv)\n",
    "    arcpy.management.Delete(f'{gdb_path}/{out_table}')\n",
    "    \n",
    "    raster = f'/path-to-data/population/ppp_{y}_1km_Aggregated.tif'\n",
    "    print(f'doing {raster}')\n",
    "    out_csv = raster.replace('.tif', '.csv')\n",
    "    arcpy.ia.ZonalStatisticsAsTable(\n",
    "        in_zone_data=admin_shp,\n",
    "        zone_field=\"MY_ID\",\n",
    "        in_value_raster=raster,\n",
    "        out_table=f'{gdb_path}/{out_table}',\n",
    "        ignore_nodata=\"DATA\",\n",
    "        statistics_type='SUM'\n",
    "    )\n",
    "    arcpy.conversion.ExportTable(f'{gdb_path}/{out_table}', out_csv)\n",
    "    arcpy.management.Delete(f'{gdb_path}/{out_table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "\n",
    "for y in years:\n",
    "    csv_1 = f'/path-to-data/population/ppp_{y}_1km_Aggregated_water.csv'\n",
    "    csv_2 = f'/path-to-data/population/ppp_{y}_1km_Aggregated.csv'\n",
    "    \n",
    "    df_1 = pd.read_csv(csv_1)\n",
    "    df_1.set_index('MY_ID')\n",
    "    df_2 = pd.read_csv(csv_2)\n",
    "    df_2.set_index('MY_ID')\n",
    "    \n",
    "    df = df_1.join(df_2, lsuffix='_w', rsuffix='_n', how='outer')\n",
    "    df['COUNT_w'] = df['COUNT_w'].fillna(0)\n",
    "    df['SUM_w'] = df['SUM_w'].fillna(0)\n",
    "    \n",
    "    with open(f'/path-to-data/population/population_full_{y}.csv', 'w', newline='') as csvfile:\n",
    "        fieldnames = ['my_id', 'year', 'count_near_water', 'pop_near_water', 'count_total', 'pop_total']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for idx, row in df.iterrows():\n",
    "            my_id = row['MY_ID_n']\n",
    "            count_near_water = row['COUNT_w']\n",
    "            pop_near_water = row['SUM_w']\n",
    "            count_total = row['COUNT_n']\n",
    "            pop_total = row['SUM_n']\n",
    "            writer.writerow({\n",
    "                'my_id': my_id,\n",
    "                'year': y,\n",
    "                'count_near_water': count_near_water,\n",
    "                'pop_near_water': pop_near_water,\n",
    "                'count_total': count_total,\n",
    "                'pop_total': pop_total\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import arcpy\n",
    "\n",
    "\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "arcpy.env.workspace = '/path-to-data/population/'\n",
    "arcpy.env.overwriteOutput = True\n",
    "stat = 'SUM'\n",
    "\n",
    "results = {}\n",
    "for y in years:\n",
    "    raster = f'ppp_{y}_1km_Aggregated.tif'\n",
    "    admin_shp = '/path-to-data/Admin2_Afro_Master_final.shp'\n",
    "    gdb_path = '/path-to-geodatabase/WHO.gdb'\n",
    "    temp_points_layer = 'temp_points'\n",
    "    missing_file_name = f'/path-to-data/population/20230723_districtsWithNoPopulationMatch.csv'\n",
    "\n",
    "    myids_not_matched = []\n",
    "    with open(missing_file_name, newline='') as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        headers = next(csvreader, None)\n",
    "        for row in csvreader:\n",
    "            myids_not_matched.append(f'\"{row[0]}\"')\n",
    "\n",
    "    tmp_clause = ','.join(myids_not_matched)\n",
    "    tmp_clause = tmp_clause.replace('\"', '\\'')\n",
    "    where_clause = f\"Admin2_Afro_Master_final.MY_ID IN ({tmp_clause})\"\n",
    "    sel_features = arcpy.management.SelectLayerByAttribute(\n",
    "        in_layer_or_view=admin_shp,\n",
    "        selection_type=\"NEW_SELECTION\",\n",
    "        where_clause=where_clause,\n",
    "        invert_where_clause=None\n",
    "    )\n",
    "    ext = arcpy.Describe(sel_features).extent\n",
    "    buf = 0.1\n",
    "    ext = arcpy.Extent(ext.XMin-buf, ext.YMin-buf, ext.XMax+buf, ext.XMax+buf)\n",
    "    with arcpy.EnvManager(extent=ext):\n",
    "        out_csv = f'population_full_{y}_3.csv'\n",
    "        if True or not os.path.exists(f'/path-to-data/population/{out_csv}'):\n",
    "            print(f'doing {raster}')\n",
    "            # total\n",
    "            arcpy.conversion.RasterToPoint(\n",
    "                in_raster=raster,\n",
    "                out_point_features=f\"{gdb_path}/{temp_points_layer}\",\n",
    "                raster_field=\"Value\"\n",
    "            )\n",
    "            result = arcpy.management.AddSpatialJoin(\n",
    "                target_features=sel_features,\n",
    "                join_features=f\"{gdb_path}/{temp_points_layer}\",\n",
    "                join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "                join_type=\"KEEP_ALL\",\n",
    "                field_mapping=f'pointid \"pointid\" true true false 4 Long 0 0,First,#,{gdb_path}/{temp_points_layer},pointid,-1,-1;grid_code \"grid_code\" true true false 4 Float 0 0,First,#,{gdb_path}/{temp_points_layer},grid_code,-1,-1',\n",
    "                match_option=\"CLOSEST_GEODESIC\",\n",
    "                search_radius=0.1,\n",
    "                distance_field_name=\"\"\n",
    "            )\n",
    "            fields = arcpy.ListFields(result)\n",
    "            find_fields = []\n",
    "            for field in fields:\n",
    "                if 'MY_ID' in field.name or 'grid_code' in field.name:\n",
    "                    find_fields.append(field.name)\n",
    "\n",
    "            result_rows = []\n",
    "            with arcpy.da.SearchCursor(result, find_fields) as cursor:\n",
    "                for row in cursor:\n",
    "                    result_rows.append([row[0], row[1]])\n",
    "\n",
    "            with open(f'/path-to-data/landcover/{year}/{out_csv}', 'w', newline='') as outcsv:\n",
    "                writer = csv.writer(outcsv)\n",
    "                writer.writerow([\"MY_ID\", stat])\n",
    "                for r in result_rows:\n",
    "                    writer.writerow(r)\n",
    "                    \n",
    "            arcpy.management.Delete(f\"{gdb_path}/{temp_points_layer}\")\n",
    "    arcpy.management.Delete(f\"{gdb_path}/sel_features_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/path-to-data/population/population_full_{y}_3.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['my_id', 'year', 'count_near_water', 'pop_near_water', 'count_total', 'pop_total']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for k, v in results.items():\n",
    "        my_id = k.split('_')[0]\n",
    "        y = k.split('_')[1]\n",
    "        count_near_water = v['count_near_water']\n",
    "        pop_near_water = v['pop_near_water']\n",
    "        count_total = v['count_total']\n",
    "        pop_total = v['pop_total']\n",
    "        writer.writerow({\n",
    "            'my_id': my_id,\n",
    "            'year': y,\n",
    "            'count_near_water': count_near_water,\n",
    "            'pop_near_water': pop_water,\n",
    "            'count_total': count_total,\n",
    "            'pop_total': pop_total\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data for the relative wealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import arcpy\n",
    "\n",
    "\n",
    "arcpy.env.workspace = f'/path-to-data/wealth/'\n",
    "arcpy.env.overwriteOutput = True\n",
    "stat = 'MEAN'\n",
    "out_table = 'temp'\n",
    "\n",
    "raster = f'/path-to-data/wealth/relative_wealth_index.tif'\n",
    "admin_shp = '/path-to-data/Admin2_Afro_Master_final.shp'\n",
    "gdb_path = '/path-to-geodatabase/WHO.gdb'\n",
    "print(f'doing {raster}')\n",
    "out_csv = raster.replace('.tif', '.csv')\n",
    "arcpy.ia.ZonalStatisticsAsTable(\n",
    "    in_zone_data=admin_shp,\n",
    "    zone_field=\"MY_ID\",\n",
    "    in_value_raster=raster,\n",
    "    out_table=f'{gdb_path}/{out_table}',\n",
    "    ignore_nodata=\"DATA\",\n",
    "    statistics_type='SUM'\n",
    ")\n",
    "arcpy.conversion.ExportTable(f'{gdb_path}/{out_table}', out_csv)\n",
    "arcpy.management.Delete(f'{gdb_path}/{out_table}')\n",
    "\n",
    "raster = f'/path-to-data/population/ppp_{y}_1km_Aggregated.tif'\n",
    "print(f'doing {raster}')\n",
    "out_csv = raster.replace('.tif', '.csv')\n",
    "arcpy.ia.ZonalStatisticsAsTable(\n",
    "    in_zone_data=admin_shp,\n",
    "    zone_field=\"MY_ID\",\n",
    "    in_value_raster=raster,\n",
    "    out_table=f'{gdb_path}/{out_table}',\n",
    "    ignore_nodata=\"DATA\",\n",
    "    statistics_type='SUM'\n",
    ")\n",
    "arcpy.conversion.ExportTable(f'{gdb_path}/{out_table}', out_csv)\n",
    "arcpy.management.Delete(f'{gdb_path}/{out_table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_csv = '/path-to-data/wealth/relative_wealth_index_full.csv'\n",
    "out_csv = '/path-to-data/wealth/relative_wealth_index_full_2.csv'\n",
    "df = pd.read_csv(in_csv)\n",
    "new_df = df[['MY_ID', 'MEAN']]\n",
    "new_df.to_csv(out_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data for elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "\n",
    "\n",
    "# Add all the dems to a mosaic\n",
    "arcpy.management.AddRastersToMosaicDataset(\n",
    "    in_mosaic_dataset=r\"/path-to-geodatabase/WHO.gdb/elevation\",\n",
    "    raster_type=\"Raster Dataset\",\n",
    "    input_path=\"/path-to-data/data\\elevation\",\n",
    "    update_cellsize_ranges=\"UPDATE_CELL_SIZES\",\n",
    "    update_boundary=\"UPDATE_BOUNDARY\",\n",
    "    update_overviews=\"NO_OVERVIEWS\",\n",
    "    maximum_pyramid_levels=None,\n",
    "    maximum_cell_size=0,\n",
    "    minimum_dimension=1500,\n",
    "    spatial_reference=None,\n",
    "    filter=\"\",\n",
    "    sub_folder=\"NO_SUBFOLDERS\",\n",
    "    duplicate_items_action=\"ALLOW_DUPLICATES\",\n",
    "    build_pyramids=\"NO_PYRAMIDS\",\n",
    "    calculate_statistics=\"NO_STATISTICS\",\n",
    "    build_thumbnails=\"NO_THUMBNAILS\",\n",
    "    operation_description=\"\",\n",
    "    force_spatial_reference=\"NO_FORCE_SPATIAL_REFERENCE\",\n",
    "    estimate_statistics=\"NO_STATISTICS\",\n",
    "    aux_inputs=None,\n",
    "    enable_pixel_cache=\"NO_PIXEL_CACHE\",\n",
    "    cache_location=r\"C:\\Users\\<windows_username>\\AppData\\Local\\ESRI\\rasterproxies\\elevation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# Reduce it to a lower resolution\n",
    "with arcpy.EnvManager(outputCoordinateSystem='GEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137.0,298.257223563]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]]', parallelProcessingFactor=\"80%\", extent='-25.3605575569999 -30.6778479549999 56.2956848150001 27.290458679 GEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137.0,298.257223563]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]]'):\n",
    "    out_raster = arcpy.sa.Aggregate(\n",
    "        in_raster=r\"/path-to-geodatabase/WHO.gdb/elevation\",\n",
    "        cell_factor=100,\n",
    "        aggregation_type=\"MEDIAN\",\n",
    "        extent_handling=\"EXPAND\",\n",
    "        ignore_nodata=\"DATA\"\n",
    "    )\n",
    "out_raster.save(r\"/path-to-geodatabase/WHO.gdb/elevation_lowres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import arcpy\n",
    "\n",
    "\n",
    "arcpy.env.workspace = '/path-to-data/elevation/'\n",
    "arcpy.env.overwriteOutput = True\n",
    "stat = 'MEDIAN'\n",
    "\n",
    "raster = '/path-to-geodatabase/WHO.gdb/elevation_lowres'\n",
    "admin_shp = '/path-to-data/Admin2_Afro_Master_final.shp'\n",
    "gdb_path = '/path-to-geodatabase/WHO.gdb'\n",
    "temp_points_layer = 'temp_points'\n",
    "missing_file_name = '/path-to-data/elevation/20230709_districtsWithNoElevationMatch.csv'\n",
    "\n",
    "myids_not_matched = []\n",
    "with open(missing_file_name, newline='') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    headers = next(csvreader, None)\n",
    "    for row in csvreader:\n",
    "        myids_not_matched.append(f'\"{row[0]}\"')\n",
    "\n",
    "tmp_clause = ','.join(myids_not_matched)\n",
    "tmp_clause = tmp_clause.replace('\"', '\\'')\n",
    "where_clause = f\"Admin2_Afro_Master_final.MY_ID IN ({tmp_clause})\"\n",
    "sel_features = arcpy.management.SelectLayerByAttribute(\n",
    "    in_layer_or_view=admin_shp,\n",
    "    selection_type=\"NEW_SELECTION\",\n",
    "    where_clause=where_clause,\n",
    "    invert_where_clause=None\n",
    ")\n",
    "ext = arcpy.Describe(sel_features).extent\n",
    "buf = 0.1\n",
    "ext = arcpy.Extent(ext.XMin-buf, ext.YMin-buf, ext.XMax+buf, ext.XMax+buf)\n",
    "with arcpy.EnvManager(extent=ext):\n",
    "    out_csv = f'elevation_full_2.csv'\n",
    "    if True or not os.path.exists(f'/path-to-data/elevation/{out_csv}'):\n",
    "        print(f'doing {raster}')\n",
    "        # total\n",
    "        arcpy.conversion.RasterToPoint(\n",
    "            in_raster=raster,\n",
    "            out_point_features=f\"{gdb_path}/{temp_points_layer}\",\n",
    "            raster_field=\"Value\"\n",
    "        )\n",
    "        result = arcpy.management.AddSpatialJoin(\n",
    "            target_features=sel_features,\n",
    "            join_features=f\"{gdb_path}/{temp_points_layer}\",\n",
    "            join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "            join_type=\"KEEP_ALL\",\n",
    "            field_mapping=f'pointid \"pointid\" true true false 4 Long 0 0,First,#,{gdb_path}/{temp_points_layer},pointid,-1,-1;grid_code \"grid_code\" true true false 4 Float 0 0,First,#,{gdb_path}/{temp_points_layer},grid_code,-1,-1',\n",
    "            match_option=\"CLOSEST_GEODESIC\",\n",
    "            search_radius=0.1,\n",
    "            distance_field_name=\"\"\n",
    "        )\n",
    "        fields = arcpy.ListFields(result)\n",
    "        find_fields = []\n",
    "        for field in fields:\n",
    "            if 'MY_ID' in field.name or 'grid_code' in field.name:\n",
    "                find_fields.append(field.name)\n",
    "\n",
    "        result_rows = []\n",
    "        with arcpy.da.SearchCursor(result, find_fields) as cursor:\n",
    "            for row in cursor:\n",
    "                result_rows.append([row[0], row[1]])\n",
    "\n",
    "        with open(f'/path-to-data/elevation/{out_csv}', 'w', newline='') as outcsv:\n",
    "            writer = csv.writer(outcsv)\n",
    "            writer.writerow([\"MY_ID\", stat])\n",
    "            for r in result_rows:\n",
    "                writer.writerow(r)\n",
    "                    \n",
    "        arcpy.management.Delete(f\"{gdb_path}/{temp_points_layer}\")\n",
    "arcpy.management.Delete(f\"{gdb_path}/sel_features_temp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
