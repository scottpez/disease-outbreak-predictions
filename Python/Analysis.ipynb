{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4984b9a-eb87-408f-a2c6-c1066baa7a06",
   "metadata": {},
   "source": [
    "# Spatial and Machine Learning Analysis for WHO-AFRO\n",
    "## Performed by Dr. Scott Pezanowski\n",
    "#### 2022-10-15 to 2023-07-31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115469f2-b232-460d-9b03-2e0f42ea4a28",
   "metadata": {},
   "source": [
    "The spatial analysis portion of my code closely follows the ESDA procedure set forth by Dr. Dani Arribes-Bel in the free online course referenced below.\n",
    "\n",
    "Arribas-Bel, D. (2019). A course on Geographic Data Science. *The Journal of Open Source Education*, *2*(14). https://doi.org/https://doi.org/10.21105/jose.00042.\n",
    "```\n",
    "@article{darribas_gds_course,\n",
    "  author = {Dani Arribas-Bel},\n",
    "  title = {A course on Geographic Data Science},\n",
    "  year = 2019,\n",
    "  journal = {The Journal of Open Source Education},\n",
    "  volume = 2,\n",
    "  number = 14,\n",
    "  doi = {https://doi.org/10.21105/jose.00042}\n",
    "}\n",
    "```\n",
    "\n",
    "http://darribas.org/gds_course/content/bF/lab_F.html\n",
    "\n",
    "The second half of my code that uses machine learning is not from Dr. Arribas-Bel's course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24453df-8ab5-4006-8c50-6ea5d9e24965",
   "metadata": {},
   "source": [
    "# Spatial autocorrelation and Exploratory Spatial Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991972f7-f1fc-44dd-ae98-6ff7c04aefda",
   "metadata": {},
   "source": [
    "Spatial autocorrelation has to do with the degree to which the similarity in values between observations in a dataset is related to the similarity in locations of such observations. Not completely unlike the traditional correlation between two variables -which informs us about how the values in one variable change as a function of those in the other- and analogous to its time-series counterpart -which relates the value of a variable at a given point in time with those in previous periods-, spatial autocorrelation relates the value of the variable of interest in a given location, with values of the same variable in surrounding locations.\n",
    "\n",
    "A key idea in this context is that of spatial randomness: a situation in which the location of an observation gives no information whatsoever about its value. In other words, a variable is spatially random if it is distributed following no discernible pattern over space. Spatial autocorrelation can thus be formally defined as the “absence of spatial randomness”, which gives room for two main classes of autocorrelation, similar to the traditional case: positive spatial autocorrelation, when similar values tend to group together in similar locations; and negative spatial autocorrelation, in cases where similar values tend to be dispersed and further apart from each other.\n",
    "\n",
    "In this session we will learn how to explore spatial autocorrelation in a given dataset, interrogating the data about its presence, nature, and strength. To do this, we will use a set of tools collectively known as Exploratory Spatial Data Analysis (ESDA), specifically designed for this purpose. The range of ESDA methods is very wide and spans from less sophisticated approaches like choropleths and general table querying, to more advanced and robust methodologies that include statistical inference and an explicit recognition of the geographical dimension of the data. The purpose of this session is to dip our toes into the latter group.\n",
    "\n",
    "ESDA techniques are usually divided into two main groups: tools to analyze *global*, and *local* spatial autocorrelation. The former consider the overall trend that the location of values follows, and makes possible statements about the degree of clustering in the dataset. *Do values generally follow a particular pattern in their geographical distribution? Are similar values closer to other similar values than we would expect from pure chance?* These are some of the questions that tools for global spatial autocorrelation allow to answer. We will practice with global spatial autocorrelation by using Moran’s I statistic.\n",
    "\n",
    "Tools for *local* spatial autocorrelation instead focus on spatial instability: the departure of parts of a map from the general trend. The idea here is that, even though there is a given trend for the data in terms of the nature and strength of spatial association, some particular areas can diverege quite substantially from the general pattern. Regardless of the overall degree of concentration in the values, we can observe pockets of unusually high (low) values close to other high (low) values, in what we will call hot(cold)spots. Additionally, it is also possible to observe some high (low) values surrounded by low (high) values, and we will name these “spatial outliers”. The main technique we will review in this session to explore local spatial autocorrelation is the Local Indicators of Spatial Association (LISA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6bb69d-231e-4028-9e68-394f724be7db",
   "metadata": {},
   "source": [
    "1. Before running this notebook, you must install AutoGluon via the instructions here https://auto.gluon.ai/stable/install.html into a Conda environment.\n",
    "2. Then, install the additional dependencies below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b28d84-6cda-4bb2-b268-a436c38b63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge pysal -y\n",
    "!conda install -c conda-forge contextily -y\n",
    "!conda install -c anaconda sqlalchemy -y\n",
    "!python -m pip install psycopg2\n",
    "!conda install -c conda-forge imbalanced-learn -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e771a9e-efe5-41cb-8ae6-f689e15d9c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pyproj\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import esda\n",
    "from pysal.lib import weights\n",
    "from splot.esda import (\n",
    "    moran_scatterplot, lisa_cluster, plot_local_autocorrelation\n",
    ")\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ts = datetime.today().strftime('%Y%m%d')\n",
    "report_image_path = '/path-to-a-directory-to-save-results/' # where to save results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2bfd82-6221-40ca-b9a3-0dd8865b642b",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae1aff9-890b-43b9-970c-bf7ae67c9d06",
   "metadata": {},
   "source": [
    "Query for the disease data from the Postgresql/PostGIS database. You can change the variable *disease* to one of the commented options to see results for a different disease.\n",
    "\n",
    "This query depends on the Postgresql/PostGIS database being created prior from the IDSR dataset, the administrative boundaries provided by Dr. Etien, a water bodies dataset extracted from Openstreetmap, and the population and relative wealth datasets downloaded from the links specified. All SQL statements and import commands are provided in separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569408d4-120b-40ca-bee8-cb3c818a7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diseases = ['cholera'] \n",
    "# diseases = ['ebola', 'ebola virus disease'] # 2023-08-02: There are not enough cases in IDSR for Ebola for proper analysis.\n",
    "# diseases = ['malaria', 'malaria (imported)', 'malaria confirmed', 'malaria tested'] # is named multiple different ways in IDSR\n",
    "# diseases = ['meningitis', 'meningococcal meningitis', 'meningococal meningitis'] # is named multiple different ways in IDSR\n",
    "diseases = ['yellow fever', 'yf'] # is named multiple different ways in IDSR\n",
    "# note that some of the diseases are named slightly differently in the IDSR dataset.\n",
    "disease = diseases[0]\n",
    "\n",
    "sql_data = f\"\"\"\n",
    "WITH projection AS (\n",
    "\tSELECT foo1.my_id,\n",
    "\tfoo1.adm0_name,\n",
    "\tfoo1.epidemic_week, \n",
    "\tfoo1.y, \n",
    "\tCOALESCE(foo3.totalcases, 0) AS totalcases FROM (\n",
    "\t\tSELECT ab.my_id, ab.adm0_name, e.epidemic_week, e.y FROM admin2_afro_master_subset1 ab\n",
    "\t\tCROSS JOIN (\n",
    "\t\t\tSELECT DISTINCT epidemic_week, y FROM idsr\n",
    "\t\t) e WHERE ab.in_idsr = 'true'\n",
    "\t) foo1\n",
    "\tLEFT JOIN \n",
    "\t(\n",
    "\t\tSELECT\t\n",
    "\t\tsum(ir.totalcases) AS totalcases,\n",
    "\t\tir.epidemic_week,\n",
    "\t\tir.y,\n",
    "\t\tir.adm2_my_id\n",
    "\t\tFROM idsr ir\n",
    "\t\tINNER JOIN diseases d\n",
    "\t\tON ir.d_id = d.id\n",
    "\t\tAND lower(d.disease) =  ANY ('{{\"{'\",\"'.join(diseases)}\"}}'::text[])\n",
    "\t\tGROUP BY adm2_my_id, epidemic_week, y\n",
    "\t) foo3\n",
    "\tON foo1.my_id = foo3.adm2_my_id AND foo1.epidemic_week = foo3.epidemic_week AND foo1.y = foo3.y\n",
    ")\n",
    "SELECT \n",
    "p.my_id,\n",
    "p.adm0_name,\n",
    "p.epidemic_week,\n",
    "p.y,\n",
    "pr.val_precipitation, \n",
    "tp.val_temperature,\n",
    "trees.val AS val_trees,\n",
    "trees.percent_cov AS percent_cov_trees,\n",
    "crops.val AS val_crops,\n",
    "crops.percent_cov AS percent_cov_crops,\n",
    "builtup.val AS val_builtup,\n",
    "builtup.percent_cov AS percent_cov_builtup,\n",
    "bareground.val AS val_bareground,\n",
    "bareground.percent_cov AS percent_cov_bareground,\n",
    "rangeland.val AS val_rangeland,\n",
    "rangeland.percent_cov AS percent_cov_rangeland,\n",
    "pf.pop_total / pf.count_total AS relative_pop_density,\n",
    "pf.pop_near_water,\n",
    "rw.val_wealth,\n",
    "floor(ef.val_elevation) AS val_elevation,\n",
    "p.totalcases\n",
    "FROM \n",
    "projection p\n",
    "LEFT JOIN precipitation_full pr\n",
    "ON p.my_id = pr.my_id AND p.epidemic_week = pr.epidemic_week\n",
    "LEFT JOIN temperature_full tp\n",
    "ON p.my_id = tp.my_id AND p.epidemic_week = tp.epidemic_week\n",
    "LEFT JOIN landcover_full trees \n",
    "ON p.my_id = trees.my_id AND p.y = trees.\"year\" AND trees.clas = 'trees'\n",
    "LEFT JOIN landcover_full crops \n",
    "ON p.my_id = crops.my_id AND p.y = crops.\"year\" AND crops.clas = 'crops'\n",
    "LEFT JOIN landcover_full builtup \n",
    "ON p.my_id = builtup.my_id AND p.y = builtup.\"year\" AND builtup.clas = 'builtup'\n",
    "LEFT JOIN landcover_full bareground \n",
    "ON p.my_id = bareground.my_id AND p.y = bareground.\"year\" AND bareground.clas = 'bareground'\n",
    "LEFT JOIN landcover_full rangeland \n",
    "ON p.my_id = rangeland.my_id AND p.y = rangeland.\"year\" AND rangeland.clas = 'rangeland'\n",
    "LEFT JOIN population_full pf\n",
    "ON p.my_id = pf.my_id AND (CASE WHEN p.y > 2020 THEN 2020 ELSE p.y END) = pf.\"year\"\n",
    "LEFT JOIN relativewealth_full rw\n",
    "ON p.my_id = rw.my_id\n",
    "LEFT JOIN elevation_full ef\n",
    "ON p.my_id = ef.my_id\n",
    "ORDER BY p.epidemic_week ASC;\n",
    "\"\"\"\n",
    "\n",
    "sql_geom = \"\"\"\n",
    " SELECT my_id::integer, geom FROM admin2_afro_master_subset1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4818c9-c957-494c-9820-9fe372728082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DB connection\n",
    "from sqlalchemy import create_engine  \n",
    "db_connection_url = \"postgresql://<dbuser>:<dbpasswd>@<dbhost>:<dbport>/<dbname>\"\n",
    "con = create_engine(db_connection_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49cb5b8-b972-43dd-8fc1-da6eb5e0cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from Postgresql and specify the datatypes for the variables to make sure they are correct\n",
    "dtypes = {\n",
    "    'my_id': np.int64,\n",
    "    'adm0_name': np.str_,\n",
    "    'val_precipitation': np.float64,\n",
    "    'val_temperature': np.float64,\n",
    "    'val_trees': np.int64,\n",
    "    'percent_cov_trees': np.float64,\n",
    "    'val_crops': np.int64,\n",
    "    'percent_cov_crops': np.float64,\n",
    "    'val_builtup': np.int64,\n",
    "    'percent_cov_builtup': np.float64,\n",
    "    'val_bareground': np.int64,\n",
    "    'percent_cov_bareground': np.float64,\n",
    "    'val_rangeland': np.int64,\n",
    "    'percent_cov_rangeland': np.float64,\n",
    "    'relative_pop_density': np.float64,\n",
    "    'pop_near_water': np.float64,\n",
    "    'val_wealth': np.float64,\n",
    "    'val_elevation': np.int64,\n",
    "    'totalcases': np.int64,\n",
    "}\n",
    "data = pd.read_sql_query(sql_data, con, dtype=dtypes, parse_dates=['epidemic_week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6aea7-a0b6-4105-afa7-36d6bf29d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140021e-0c78-4ea7-960a-e89f838279ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure there are no null values\n",
    "len(data[data.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8734c7-9094-4b9c-bce7-980314d87dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data[data['totalcases'] > 0]))\n",
    "print(len(data[data['totalcases'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816fd8ac-69f7-419b-bef7-3646aacc201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['totalcases'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79555437-26f8-4653-9476-e6dc8cc3b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the boundaries of the administrative districts so that we can produce map visualizations\n",
    "geo = gpd.read_postgis(sql_geom, con, geom_col='geom', index_col='my_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450a3b9-68a3-4f3b-bce4-d807eab1ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'val_precipitation': 'mean',\n",
    "    'val_temperature': 'mean',\n",
    "    'val_trees': 'mean',\n",
    "    'percent_cov_trees': 'mean',\n",
    "    'val_crops': 'mean',\n",
    "    'percent_cov_crops': 'mean',\n",
    "    'val_builtup': 'mean',\n",
    "    'percent_cov_builtup': 'mean',\n",
    "    'val_bareground': 'mean',\n",
    "    'percent_cov_bareground': 'mean',\n",
    "    'val_rangeland': 'mean',\n",
    "    'percent_cov_rangeland': 'mean',\n",
    "    'relative_pop_density': 'mean',\n",
    "    'pop_near_water': 'mean',\n",
    "    'val_wealth': 'mean',\n",
    "    'val_elevation': 'mean',\n",
    "    'totalcases': 'sum',\n",
    "}\n",
    "data_agg = data.groupby(['my_id', 'adm0_name'], as_index = False).agg(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7dec36-a0eb-4efc-8a49-f3ec4cb32a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c3670e-b515-4db2-867b-5fce3e31c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the disease data with the boundaries\n",
    "geo2 = geo.merge(data_agg, on='my_id', how='left')\n",
    "geo2['totalcases'] = geo2['totalcases'].astype(int)\n",
    "geo2['adm0_name'] = geo2['adm0_name'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4a54da-9889-4455-a66e-ace03d6f9d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4693de18-abed-4091-88ec-fa288fc18bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in some geographic datasets to help with the map visualizations\n",
    "waterbodies_gdf = gpd.read_file(\"/path-to-data/africa_waterbody.gpkg\")\n",
    "africa_gdf = gpd.read_file(\"/path-to-data/africa.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dca476-ed0c-4eb6-a3a8-ab550c8ab98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the summary counts on a map\n",
    "ax = africa_gdf.plot(color='beige', figsize=(20,18))\n",
    "geo2.plot(ax=ax, color=\"lightgrey\")\n",
    "lbl = f\"Total {disease} cases\"\n",
    "geo2[geo2['totalcases'] > 0].plot(ax=ax, column='totalcases', scheme='FisherJenks', k=7, legend=True)\n",
    "leg1 = ax.get_legend()\n",
    "leg1.set_title(f'Total cases for {disease}')\n",
    "ax.set_axis_off()\n",
    "ax.figure.savefig(f'{report_image_path}{os.sep}{ts}_{disease}_summarymap.png', bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e51b2-8d5c-4858-af41-a7004e80b198",
   "metadata": {},
   "source": [
    "Now let’s index it on the local authority IDs, while keeping those as a column too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9d047-f196-48eb-8c86-4280b66d92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I commented out these lines because I explicitly set the index in the read_sql_query function above\n",
    "# Index table on the LAD ID\n",
    "# geo2 = geo2.set_index(\"my_id\", inplace=True)\n",
    "geo2.reset_index(inplace=True)\n",
    "# Display summary\n",
    "geo2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df268bd-808a-44af-847d-cde4529177ef",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc7be88-72b1-4a05-a70e-d014ca99289a",
   "metadata": {},
   "source": [
    "Let’s get a first view of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c48bd2-8d1c-44d6-9444-56ba896b7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot polygons\n",
    "ax = geo2.plot(alpha=0.5, color='red')\n",
    "# Add background map, expressing target CRS so the basemap can be\n",
    "# reprojected (warped)\n",
    "ctx.add_basemap(ax, crs=geo2.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f9327-f83d-4994-bb8c-ed6aefd94241",
   "metadata": {},
   "source": [
    "### Spatial weights matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5c16a-22a3-4180-b6f5-03ab6b07755d",
   "metadata": {},
   "source": [
    "As discused before, a spatial weights matrix is the way geographical space is formally encoded into a numerical form so it is easy for a computer (or a statistical method) to understand. We have seen already many of the conceptual ways in which we can define a spatial weights matrix, such as contiguity, distance-based, or block.\n",
    "\n",
    "For this example, we will show how to build a queen contiguity matrix, which considers two observations as neighbors if they share at least one point of their boundary. In other words, for a pair of local authorities in the dataset to be considered neighbours under this W, they will need to be sharing border or, in other words, “touching” each other to some degree.\n",
    "\n",
    "Technically speaking, we will approach building the contiguity matrix in the same way we did in Lab 5. We will begin with a GeoDataFrame and pass it on to the queen contiguity weights builder in PySAL (ps.weights.Queen.from_dataframe). We will also make sure our table of data is previously indexed on the local authority code, so the W is also indexed on that form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f980ec5-4d82-41dd-ba88-97ba8de9a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spatial weights matrix\n",
    "%time w = weights.Queen.from_dataframe(geo2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad362e-fb2d-4be7-8aa4-02189e700486",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.islands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc772bb-36e8-477d-9563-23ff079f6fc0",
   "metadata": {},
   "source": [
    "#### There are some islands. Therefore, let's connect those using the nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d038d12-0ad4-47be-9665-6547f05f8ad8",
   "metadata": {},
   "source": [
    "I removed the lines below about accounting for islands because it caused problems with the spatial correlation. The islands for our analysis are actually physical islands off the coast of Africa. Therefore, it does not make sense to include them in the spatial correlation analysis. Any sort of spatial correlation that includes them would require a more complicated way to include their relationships with other districts on the mainland."
   ]
  },
  {
   "cell_type": "raw",
   "id": "af450c44-b165-4cd3-93d3-e68331347054",
   "metadata": {},
   "source": [
    "%time w_d = weights.distance.KNN.from_dataframe(geo2, k=1, ids=list(geo2['my_id']))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f80e19b1-0a14-41ee-9a72-755b9b5a3d0a",
   "metadata": {},
   "source": [
    "neighbors = w.neighbors.copy()\n",
    "\n",
    "for i in w.islands:\n",
    "    neighbors[i].append(w_d.neighbors[i][0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa7fa06e-3274-48d5-875e-57576c035c67",
   "metadata": {},
   "source": [
    "w_new = weights.W(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655066d-6a48-4909-bd63-bdc755ba58af",
   "metadata": {},
   "source": [
    "#### Next, I calculate clock weights which gives a weight to all districts in the same country.\n",
    "\n",
    "I am doing this because I am making the assumption that people will not move as much between countries as within countries. Therefore, farther down, if the Queen neighbors fall in the same country, I give them a greater weight."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d49c978-4239-4702-bbf6-16542b9656ff",
   "metadata": {},
   "source": [
    "w_bl = weights.util.block_weights(\n",
    "    geo2[\"adm0_name\"].values,\n",
    "    ids=geo2[\"my_id\"].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "472732ec-ae3d-43f8-bf87-c4738e68301b",
   "metadata": {},
   "source": [
    "not_in_country_weight_divider = 5 # I arbitrarily decided on this value. It means that if the neighbor district is in another country, its wieght will be divided by this value.\n",
    "\n",
    "for k, v in w_new.neighbors.items():\n",
    "    for i in range(len(v)):\n",
    "        if not v[i] in w_bl.neighbors[k]:\n",
    "            old_v = w_new.weights[k][i]\n",
    "            w_new.weights[k][i] = old_v / not_in_country_weight_divider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa65c6c-fd8b-4dcf-ba26-7231a0db96a8",
   "metadata": {},
   "source": [
    "Now, the w object we have just is of the same type of any other one we have created in the past. As such, we can inspect it in the same way. For example, we can check who is a neighbor of observation 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2a3ce-56bc-4064-be67-51932f546b9e",
   "metadata": {},
   "source": [
    "However, the cell where we computed W returned a warning on “islands”. Remember these are islands not necessarily in the geographic sense (although some of them will be), but in the mathematical sense of the term: local authorities that are not sharing border with any other one and thus do not have any neighbors. We can inspect and map them to get a better sense of what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66264d-2cb4-4153-a700-f59f5fe81b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot polygons\n",
    "ax = geo2.loc[w.islands].plot(alpha=0.5, color='red')\n",
    "# Add background map, expressing target CRS so the basemap can be\n",
    "# reprojected (warped)\n",
    "ctx.add_basemap(ax, crs=geo2.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d4f0d-07ed-4e38-a35b-9274de0f015c",
   "metadata": {},
   "source": [
    "* Again, note that the below is the case for us that our islands are actual islands and not errors in the geometry.\n",
    "\n",
    "In this case, all the islands are indeed “real” islands. These cases can create issues in the analysis and distort the results. There are several solutions to this situation such as connecting the islands to other observations through a different criterium (e.g. nearest neighbor), and then combining both spatial weights matrices. For convenience, we will remove them from the dataset because they are a small sample and their removal is likely not to have a large impact in the calculations.\n",
    "\n",
    "Technically, this amounts to a subsetting, very much like we saw in the first weeks of the course, although in this case we will use the drop command, which comes in very handy in these cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32fd18f-6f0a-48c2-a6d0-e6de6c4bc3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo2 = geo2.drop(w.islands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9e007-d8e9-418c-aba1-c23ed9effc5f",
   "metadata": {},
   "source": [
    "Once we have the set of local authorities that are not an island, we need to re-calculate the weights matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7d00a-9da9-4d82-847d-2c190ba0b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spatial weights matrix\n",
    "# NOTE: this might take a few minutes as the geometries are\n",
    "#       are very detailed\n",
    "%time w = weights.Queen.from_dataframe(geo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50190cd-d89a-4a0b-a24b-acd1810d7e2f",
   "metadata": {},
   "source": [
    "And, finally, let us row-standardize it to make sure every row of the matrix sums up to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a993f-f9cd-47dc-9301-5fabf13230c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row standardize the matrix\n",
    "# w_new.transform = 'R'\n",
    "w.transform = 'R'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360fa51-03f1-489c-ab49-74e9ba95f1bf",
   "metadata": {},
   "source": [
    "Now, because we have row-standardize them, the weight given to each of the three neighbors is 0.33 which, all together, sum up to one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fbad65-5e34-4139-aab3-b99312a80c62",
   "metadata": {},
   "source": [
    "### Spatial Lag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3543dcf-5d28-470f-802a-2606fc03d9fe",
   "metadata": {},
   "source": [
    "Once we have the data and the spatial weights matrix ready, we can start by computing the spatial lag of the total cases of the disease. Remember the spatial lag is the product of the spatial weights matrix and a given variable and that, if W is row-standardized, the result amounts to the average value of the variable in the neighborhood of each observation.\n",
    "\n",
    "We can calculate the spatial lag for the variable total_cases and store it directly in the main table with the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e9125-7ff1-4131-abd0-faa044ce459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo2['w_totalcases'] = weights.lag_spatial(w, geo2['totalcases'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a0e0e-5a13-47aa-9228-941918d856c9",
   "metadata": {},
   "source": [
    "Let us have a quick look at the resulting variable, as compared to the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdb5a3-296a-4389-9c3c-b9fa6c64c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo2[['my_id', 'totalcases', 'w_totalcases']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd2ca6-a5a0-4d7c-9bbe-aadb5968cc33",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Global Spatial autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c9719-ceeb-4bdf-975d-d80aa8fdd20d",
   "metadata": {},
   "source": [
    "Global spatial autocorrelation relates to the overall geographical pattern present in the data. Statistics designed to measure this trend thus characterize a map in terms of its degree of clustering and summarize it. This summary can be visual or numerical. In this section, we will walk through an example of each of them: the Moran Plot, and Moran’s I statistic of spatial autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81703e7-eb8b-40c7-bc77-1d7739623020",
   "metadata": {},
   "source": [
    "### Moran Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7cfe4-6de8-4dd1-a8c3-1d53ffaf7348",
   "metadata": {},
   "source": [
    "The moran plot is a way of visualizing a spatial dataset to explore the nature and strength of spatial autocorrelation. It is essentially a traditional scatter plot in which the variable of interest is displayed against its spatial lag. In order to be able to interpret values as above or below the mean, and their quantities in terms of standard deviations, the variable of interest is usually standardized by substracting its mean and dividing it by its standard deviation.\n",
    "\n",
    "Technically speaking, creating a Moran Plot is very similar to creating any other scatter plot in Python, provided we have standardized the variable and calculated its spatial lag beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69ecc6-de18-42c7-a9aa-064252da75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Plot values\n",
    "sns.regplot(x='totalcases', y='w_totalcases', data=geo2, ci=None)\n",
    "# Add vertical and horizontal lines\n",
    "plt.axvline(0, c='k', alpha=0.5)\n",
    "plt.axhline(0, c='k', alpha=0.5)\n",
    "# Display\n",
    "plt.savefig(f'{report_image_path}{os.sep}{ts}_{disease}_summaryspatialcorrelationplot.png', bbox_inches='tight', transparent=True, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3987f7e-5730-4bad-ad70-80cf05f54b50",
   "metadata": {},
   "source": [
    "The figure above displays the relationship between the standardized total cases and its spatial lag which, because the W that was used is row-standardized, can be interpreted as the average cases in the surrounding areas of a given Local Authority. In order to guide the interpretation of the plot, a linear fit is also included in the post. This line represents the best linear fit to the scatter plot or, in other words, what is the best way to represent the relationship between the two variables as a straight line.\n",
    "\n",
    "The plot displays a positive relationship between both variables. This is associated with the presence of positive spatial autocorrelation: similar values tend to be located close to each other. This means that the overall trend is for high values to be close to other high values, and for low values to be surrounded by other low values. This however does not mean that this is only situation in the dataset: there can of course be particular cases where high values are surrounded by low ones, and viceversa. But it means that, if we had to summarize the main pattern of the data in terms of how clustered similar values are, the best way would be to say they are positively correlated and, hence, clustered over space.\n",
    "\n",
    "In the context of the example, this can be interpreted along the lines of: local authorities display positive spatial autocorrelation in the number of total cases. This means that local authorities with high percentage of total cases tend to be located nearby other local authorities where there are high total cases, and viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b87fd-777c-405d-a36d-9b020f1bc004",
   "metadata": {},
   "source": [
    "### Moran's I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65169bd-4c56-405d-8d21-25a954353b29",
   "metadata": {},
   "source": [
    "The Moran Plot is an excellent tool to explore the data and get a good sense of how much values are clustered over space. However, because it is a graphical device, it is sometimes hard to condense its insights into a more concise way. For these cases, a good approach is to come up with a statistical measure that summarizes the figure. This is exactly what Moran’s I is meant to do.\n",
    "\n",
    "Very much in the same way the mean summarizes a crucial element of the distribution of values in a non-spatial setting, so does Moran’s I for a spatial dataset. Continuing the comparison, we can think of the mean as a single numerical value summarizing a histogram or a kernel density plot. Similarly, Moran’s I captures much of the essence of the Moran Plot. In fact, there is an even close connection between the two: the value of Moran’s I corresponds with the slope of the linear fit overlayed on top of the Moran Plot.\n",
    "\n",
    "In order to calculate Moran’s I in our dataset, we can call a specific function in PySAL directly.\n",
    "\n",
    "Note how we do not need to use the standardized version in this context as we will not represent it visually.\n",
    "\n",
    "The method ps.Moran creates an object that contains much more information than the actual statistic. If we want to retrieve the value of the statistic, we can do it this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedcb7e0-cf7c-4dca-ba93-d61c4e35e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = esda.Moran(geo2['totalcases'], w)\n",
    "print(mi.I)\n",
    "print(mi.EI_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cead850-71c3-4d21-9ed6-e5c7675407f0",
   "metadata": {},
   "source": [
    "The other bit of information we will extract from Moran’s I relates to statistical inference: how likely is the pattern we observe in the map and Moran’s I captures in its value to be generated by an entirely random process? If we considered the same variable but shuffled its locations randomly, would we obtain a map with similar characteristics?\n",
    "\n",
    "The specific details of the mechanism to calculate this are beyond the scope of the session, but it is important to know that a small enough p-value associated with the Moran’s I of a map allows to reject the hypothesis that the map is random. In other words, we can conclude that the map displays more spatial pattern that we would expect if the values had been randomly allocated to a particular location.\n",
    "\n",
    "The most reliable p-value for Moran’s I can be found in the attribute p_sim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f69e14-2e18-48fa-8a7b-2c1e564464d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mi.p_sim)\n",
    "print(mi.z_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa960af0-adc6-45a3-9bca-bef1c3027df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_stats_file = f'{report_image_path}{os.sep}{ts}_summaryspatialcorrelationstats.csv'\n",
    "file_exists = os.path.isfile(spatial_stats_file)\n",
    "with open(spatial_stats_file, 'a') as f:\n",
    "    if not file_exists:\n",
    "        f.write(\"Disease,Moran's I,EI_sim,p_sim,z_sim\\n\")\n",
    "    f.write(f'{disease},{round(mi.I, 4)},{round(mi.EI_sim, 4)},{mi.p_sim},{round(mi.z_sim, 4)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff273c-7dad-4662-8270-f9f9001c721f",
   "metadata": {},
   "source": [
    "That is just 0.1% and, by standard terms, it would be considered statistically significant. We can quickly ellaborate on its intuition. What that 0.001 (or 0.1%) means is that, if we generated a large number of maps with the same values but randomly allocated over space, and calculated the Moran’s I statistic for each of those maps, only 0.1% of them would display a larger (absolute) value than the one we obtain from the real data, and the other 99.9% of the random maps would receive a smaller (absolute) value of Moran’s I. If we remember again that the value of Moran’s I can also be interpreted as the slope of the Moran Plot, what we have is that, in this case, the particular spatial arrangement of values for cases is more concentrated than if the values had been allocated following a completely spatially random process, hence the statistical significance.\n",
    "\n",
    "Once we have calculated Moran’s I and created an object like mi, we can use some of the functionality in splot to replicate the plot above more easily (remember, D.R.Y.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a071f-4ac5-40ce-a142-c9df0bedaf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "moran_scatterplot(mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe171bd-6902-44a4-9713-8719b2b8a8e9",
   "metadata": {},
   "source": [
    "As a first step, the global autocorrelation analysis can teach us that observations do seem to be positively correlated over space. In terms of our initial goal to find spatial structure in the disease cases, this view seems to align: if the cases had no such structure, it should not show a pattern over space -technically, it would show a random one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00917167-3656-4c1a-9aea-be108973b487",
   "metadata": {},
   "source": [
    "## Local Spatial autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b674a8a-73eb-432d-ae7a-8f569a281bc8",
   "metadata": {},
   "source": [
    "Moran’s I is good tool to summarize a dataset into a single value that informs about its degree of clustering. However, it is not an appropriate measure to identify areas within the map where specific values are located. In other words, Moran’s I can tell us values are clustered overall, but it will not inform us about where the clusters are. For that purpose, we need to use a local measure of spatial autocorrelation. Local measures consider each single observation in a dataset and operate on them, as oposed to on the overall data, as global measures do. Because of that, they are not good a summarizing a map, but they allow to obtain further insight.\n",
    "\n",
    "In this session, we will consider Local Indicators of Spatial Association (LISAs), a local counter-part of global measures like Moran’s I. At the core of these method is a classification of the observations in a dataset into four groups derived from the Moran Plot: high values surrounded by high values (HH), low values nearby other low values (LL), high values among low values (HL), and viceversa (LH). Each of these groups are typically called “quadrants”. An illustration of where each of these groups fall into the Moran Plot can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5888dc-7252-42d8-9ead-6e2a2f9229bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(8, 8))\n",
    "# Plot values\n",
    "sns.regplot(x='totalcases', y='w_totalcases', data=geo2, ci=None)\n",
    "# Add vertical and horizontal lines\n",
    "plt.axvline(0, c='k', alpha=0.5)\n",
    "plt.axhline(0, c='k', alpha=0.5)\n",
    "plt.text(1.75, 0.5, \"HH\", fontsize=25)\n",
    "plt.text(1.5, -1.5, \"HL\", fontsize=25)\n",
    "plt.text(-2, 1, \"LH\", fontsize=25)\n",
    "plt.text(-1.5, -2.5, \"LL\", fontsize=25)\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44be6b5-d555-4e22-bac4-d1724bc572b6",
   "metadata": {},
   "source": [
    "So far we have classified each observation in the dataset depending on its value and that of its neighbors. This is only half way into identifying areas of unusual concentration of values. To know whether each of the locations is a statistically significant cluster of a given kind, we again need to compare it with what we would expect if the data were allocated in a completely random way. After all, by definition, every observation will be of one kind of another, based on the comparison above. However, what we are interested in is whether the strength with which the values are concentrated is unusually high.\n",
    "\n",
    "This is exactly what LISAs are designed to do. As before, a more detailed description of their statistical underpinnings is beyond the scope in this context, but we will try to shed some light into the intuition of how they go about it. The core idea is to identify cases in which the comparison between the value of an observation and the average of its neighbors is either more similar (HH, LL) or dissimilar (HL, LH) than we would expect from pure chance. The mechanism to do this is similar to the one in the global Moran’s I, but applied in this case to each observation, resulting then in as many statistics as original observations.\n",
    "\n",
    "LISAs are widely used in many fields to identify clusters of values in space. They are a very useful tool that can quickly return areas in which values are concentrated and provide suggestive evidence about the processes that might be at work. For that, they have a prime place in the exploratory toolbox. Examples of contexts where LISAs can be useful include: identification of spatial clusters of poverty in regions, detection of ethnic enclaves, delineation of areas of particularly high/low activity of any phenomenon, etc.\n",
    "\n",
    "In Python, we can calculate LISAs in a very streamlined way thanks to PySAL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046fb69-080c-48cc-82dd-020d75a3e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "lisa = esda.Moran_Local(geo2['totalcases'], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56258a0-0eba-4aaf-99e1-dc3416a10229",
   "metadata": {},
   "source": [
    "All we need to pass is the variable of interest -total cases- and the spatial weights that describes the neighborhood relations between the different observation that make up the dataset.\n",
    "\n",
    "Because of their very nature, looking at the numerical result of LISAs is not always the most useful way to exploit all the information they can provide. Remember that we are calculating a statistic for every sigle observation in the data so, if we have many of them, it will be difficult to extract any meaningful pattern. Instead, what is typically done is to create a map, a cluster map as it is usually called, that extracts the significant observations (those that are highly unlikely to have come from pure chance) and plots them with a specific color depending on their quadrant category.\n",
    "\n",
    "All of the needed pieces are contained inside the lisa object we have created above. But, to make the map making more straightforward, it is convenient to pull them out and insert them in the main data table, br:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f8edd-a91c-41f4-bc1e-09daf2e32b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break observations into significant or not\n",
    "geo2['significant'] = lisa.p_sim < 0.05\n",
    "# Store the quadrant they belong to\n",
    "geo2['quadrant'] = lisa.q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1147d-cc35-46df-8e33-bab160699b97",
   "metadata": {},
   "source": [
    "Let us stop for second on these two steps. First, the significant column. Similarly as with global Moran’s I, PySAL is automatically computing a p-value for each LISA. Because not every observation represents a statistically significant one, we want to identify those with a p-value small enough that rules out the possibility of obtaining a similar situation from pure chance. Following a similar reasoning as with global Moran’s I, we select 5% as the threshold for statistical significance. To identify these values, we create a variable, significant, that contains True if the p-value of the observation is satisfies the condition, and False otherwise. We can check this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a88d0-47c9-4ec2-b6bf-55296323c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo2['significant'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb138d-aa82-4509-933e-84339f69f10a",
   "metadata": {},
   "source": [
    "And the first five p-values can be checked by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a51c3d-9ee0-4a39-97bc-00f97966e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lisa.p_sim[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18333e62-0a9a-4eaa-9db0-9695bfea3962",
   "metadata": {},
   "source": [
    "Note how the third and fourth are smaller than 0.05, as the variable significant correctly identified.\n",
    "\n",
    "Second, the quadrant each observation belongs to. This one is easier as it comes built into the lisa object directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe295efd-aee6-4abc-8355-a42fe89c52b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo2['quadrant'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f03ff-4660-4004-a835-1fa07afc47f8",
   "metadata": {},
   "source": [
    "The correspondence between the numbers in the variable and the actual quadrants is as follows:\n",
    "\n",
    "1: HH\n",
    "\n",
    "2: LH\n",
    "\n",
    "3: LL\n",
    "\n",
    "4: HL\n",
    "\n",
    "With these two elements, significant and quadrant, we can build a typical LISA cluster map combining the mapping skills with what we have learned about subsetting and querying tables:\n",
    "\n",
    "We can create a quick LISA cluster map with splot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6397ff6a-e9fe-4c02-900e-6c3ca072f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{report_image_path}{ts}_{disease}_summaryhotspots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c3c8c-ebb1-453a-92fe-be47f90f45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# f, ax = plt.subplots(1, figsize=(20, 20))\n",
    "# f.patch.set_facecolor('#BFBFBF')\n",
    "ax = africa_gdf.plot(color='beige', figsize=(20,20))\n",
    "geo2.plot(ax=ax, color=\"lightgrey\")\n",
    "lisa_cluster(lisa, geo2, ax=ax)\n",
    "# notincluded_gdf.plot(ax=ax, color='beige')\n",
    "waterbodies_gdf.plot(ax=ax, color='black')\n",
    "leg1 = ax.get_legend()\n",
    "leg1.set_title(f'Hot spots for {disease}')\n",
    "ax.set_axis_off()\n",
    "plt.savefig(f'{report_image_path}{ts}_{disease}_summaryhotspots.png', bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c40db-5f34-440f-a911-40dea774d2fc",
   "metadata": {},
   "source": [
    "Or, if we want to have more control over what is being displayed, and how each component is presented, we can “cook” the plot ourselves:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d996937-2c14-4bfc-bc6f-521aff5ce8cb",
   "metadata": {},
   "source": [
    "Below is the same plot but showing how you can change colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c3c18-3a2d-4bda-9668-1713a4f93070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Plot insignificant clusters\n",
    "ns = geo2.loc[geo2['significant']==False, 'geom']\n",
    "ns.plot(ax=ax, color='grey')\n",
    "# Plot HH clusters\n",
    "hh = geo2.loc[(geo2['quadrant']==1) & (geo2['significant']==True), 'geom']\n",
    "hh.plot(ax=ax, color='red')\n",
    "# Plot LL clusters\n",
    "ll = geo2.loc[(geo2['quadrant']==3) & (geo2['significant']==True), 'geom']\n",
    "ll.plot(ax=ax, color='mediumblue')\n",
    "# Plot LH clusters\n",
    "lh = geo2.loc[(geo2['quadrant']==2) & (geo2['significant']==True), 'geom']\n",
    "lh.plot(ax=ax, color='lightblue')\n",
    "# Plot HL clusters\n",
    "hl = geo2.loc[(geo2['quadrant']==4) & (geo2['significant']==True), 'geom']\n",
    "hl.plot(ax=ax, color='salmon')\n",
    "waterbodies_gdf.plot(ax=ax, color='black')\n",
    "# Style and draw\n",
    "f.suptitle('Total cases', size=30)\n",
    "f.set_facecolor('0.75')\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611f549-e22a-428d-9ba1-30c84c90b90c",
   "metadata": {},
   "source": [
    "The map above displays the LISA results of the disease totasl cases. In bright red, we find those local authorities with an unusual concentration of high cases surrounded also by high levels of cases. In light red, we find the first type of spatial outliers. These are areas with high cases but surrounded by areas with low cases. Finally, in light blue we find the other type of spatial outlier: local authorities with low cases surrounded by other authorities with high cases.\n",
    "\n",
    "The substantive interpretation of a LISA map needs to relate its output to the original intention of the analyst who created the map. In this case, our original idea was to explore the spatial structure of cases. The LISA proves a fairly useful tool in this context. Comparing the LISA map above with the choropleth we started with, we can interpret the LISA as “simplification” of the detailed but perhaps too complicated picture in the choropleth that focuses the reader’s attention to the areas that display a particularly high concentration of (dis)similar values, helping the spatial structure of the cases emerge in a more explicit way.\n",
    "\n",
    "The results from the LISA statistics can be connected to the Moran plot to visualise where in the scatter plot each type of polygon falls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07702049-af21-413e-b7ba-19874ae25714",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_local_autocorrelation(lisa, geo2, 'totalcases', figsize=(20,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f981a-ca45-421c-b2fb-b0da2afe533f",
   "metadata": {},
   "source": [
    "## Correlation of total cases and population near water bodies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a165c-919f-40fd-9306-4cf022a85313",
   "metadata": {},
   "source": [
    "Here, I calculate a simple linear correlation of the total cases per district with the other variables in that same district. We can see that there is a slight global significant linear correlation between the variables and cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1653637-8467-45af-bb86-b8e78a81cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data['totalcases'].corr(data['val_precipitation'])\n",
    "print(f'Linear correlation for precipitation and total_cases is: {corr}')\n",
    "corr = data['totalcases'].corr(data['val_temperature'])\n",
    "print(f'Linear correlation for temperature and total_cases is: {corr}')\n",
    "corr = data['totalcases'].corr(data['val_trees'])\n",
    "print(f'Linear correlation for val_trees and total_cases is: {corr}')\n",
    "corr = data['totalcases'].corr(data['percent_cov_trees'])\n",
    "print(f'Linear correlation for percent_cov_trees and total_cases is: {corr}')\n",
    "corr = data['totalcases'].corr(data['val_crops'])\n",
    "print(f'Linear correlation for val_crops and total_cases is: {corr}')\n",
    "corr = data['totalcases'].corr(data['percent_cov_crops'])\n",
    "print(f'Linear correlation for percent_cov_crops and total_cases is: {corr}')\n",
    "corr = data['totalcases'].corr(data['val_builtup'])\n",
    "print(f'Linear correlation for val_builtup and total_cases is: {corr}')\n",
    "corr = data['totalcases'].corr(data['percent_cov_builtup'])\n",
    "print(f'Linear correlation for percent_cov_builtup and total_cases is: {corr}')\n",
    "corr = data['totalcases'].corr(data['val_bareground'])\n",
    "print(f'Linear correlation for val_bareground and total_cases is: {corr}')\n",
    "corr = data['totalcases'].corr(data['percent_cov_bareground'])\n",
    "print(f'Linear correlation for percent_cov_bareground and total_cases is: {corr}')\n",
    "corr = data['totalcases'].corr(data['val_rangeland'])\n",
    "print(f'Linear correlation for val_rangeland and total_cases is: {corr}')\n",
    "corr = data['totalcases'].corr(data['percent_cov_rangeland'])\n",
    "print(f'Linear correlation for percent_cov_rangeland and total_cases is: {corr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a1ae0-3e11-4350-9ec4-33eb7c9ca029",
   "metadata": {},
   "source": [
    "Scatterplots of the total cases against the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a832723d-9d3f-4e5b-b1c7-ca5f52cd1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(6, 2, figsize=(12, 25))\n",
    "data.plot.scatter(x = 'val_precipitation', y = 'totalcases', ax = axs[0, 0], c='DarkBlue')\n",
    "data.plot.scatter(x = 'val_temperature', y = 'totalcases', ax = axs[0, 1], c='DarkBlue')\n",
    "data.plot.scatter(x = 'val_trees', y = 'totalcases', ax = axs[1, 0], c='DarkBlue')\n",
    "data.plot.scatter(x = 'percent_cov_trees', y = 'totalcases', ax = axs[1, 1], c='DarkBlue')\n",
    "data.plot.scatter(x = 'val_crops', y = 'totalcases', ax = axs[2, 0], c='DarkBlue')\n",
    "data.plot.scatter(x = 'percent_cov_crops', y = 'totalcases', ax = axs[2, 1], c='DarkBlue')\n",
    "data.plot.scatter(x = 'val_builtup', y = 'totalcases', ax = axs[3, 0], c='DarkGreen')\n",
    "data.plot.scatter(x = 'percent_cov_builtup', y = 'totalcases', ax = axs[3, 1], c='DarkRed')\n",
    "data.plot.scatter(x = 'val_bareground', y = 'totalcases', ax = axs[4, 0], c='DarkBlue')\n",
    "data.plot.scatter(x = 'percent_cov_bareground', y = 'totalcases', ax = axs[4, 1], c='DarkBlue')\n",
    "data.plot.scatter(x = 'val_rangeland', y = 'totalcases', ax = axs[5, 0], c='DarkGreen')\n",
    "data.plot.scatter(x = 'percent_cov_rangeland', y = 'totalcases', ax = axs[5, 1], c='DarkRed')\n",
    "plt.savefig(f'{report_image_path}{os.sep}{ts}_{disease}_scatterplots.png', bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b58d86-a075-4fa3-8257-518f048042b0",
   "metadata": {},
   "source": [
    "Below, I tried fitting a polynomial line of 3 and 5 order to the data and visualzie the results in the graph. The graphs suggest that by using correlations other than linear and by using machine learning models, there is good potential to predict cases from the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a26b8-6e7c-416f-a069-27512d11c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6, 2, figsize=(8, 25))\n",
    "\n",
    "sns.regplot(data=data_agg, ax=axs[0,0], x='totalcases', y='val_precipitation', order=2, scatter_kws={\"color\": \"LightBlue\"}, line_kws={\"color\": \"grey\"})\n",
    "sns.regplot(data=data_agg, ax=axs[0,1], x='totalcases', y='val_temperature', order=2, scatter_kws={\"color\": \"LightBlue\"}, line_kws={\"color\": \"grey\"})\n",
    "sns.regplot(data=data_agg, ax=axs[1,0], x='totalcases', y='val_trees', order=2, scatter_kws={\"color\": \"LightBlue\"}, line_kws={\"color\": \"grey\"})\n",
    "sns.regplot(data=data_agg, ax=axs[1,1], x='totalcases', y='percent_cov_trees', order=2, scatter_kws={\"color\": \"LightBlue\"}, line_kws={\"color\": \"grey\"})\n",
    "sns.regplot(data=data_agg, ax=axs[2,0], x='totalcases', y='val_crops', order=2, scatter_kws={\"color\": \"LightBlue\"}, line_kws={\"color\": \"grey\"})\n",
    "sns.regplot(data=data_agg, ax=axs[2,1], x='totalcases', y='percent_cov_crops', order=2, scatter_kws={\"color\": \"LightBlue\"}, line_kws={\"color\": \"grey\"})\n",
    "sns.regplot(data=data_agg, ax=axs[3,0], x='totalcases', y='val_builtup', order=2, scatter_kws={\"color\": \"LightBlue\"}, line_kws={\"color\": \"grey\"})\n",
    "sns.regplot(data=data_agg, ax=axs[3,1], x='totalcases', y='percent_cov_builtup', order=2, scatter_kws={\"color\": \"LightBlue\"}, line_kws={\"color\": \"grey\"})\n",
    "sns.regplot(data=data_agg, ax=axs[4,0], x='totalcases', y='val_bareground', order=2, scatter_kws={\"color\": \"LightBlue\"}, line_kws={\"color\": \"grey\"})\n",
    "sns.regplot(data=data_agg, ax=axs[4,1], x='totalcases', y='percent_cov_bareground', order=2, scatter_kws={\"color\": \"LightBlue\"}, line_kws={\"color\": \"grey\"})\n",
    "sns.regplot(data=data_agg, ax=axs[5,0], x='totalcases', y='val_rangeland', order=2, scatter_kws={\"color\": \"LightBlue\"}, line_kws={\"color\": \"grey\"})\n",
    "sns.regplot(data=data_agg, ax=axs[5,1], x='totalcases', y='percent_cov_rangeland', order=2, scatter_kws={\"color\": \"LightBlue\"}, line_kws={\"color\": \"grey\"})\n",
    "plt.savefig(f'{report_image_path}{os.sep}{ts}_{disease}_summaryregressionplots.png', bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b12354-daae-41b9-94ad-f302220a7130",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e14661-9151-4992-98b6-f763030abeee",
   "metadata": {},
   "source": [
    "The boxplot shows that there are many outliers and a lot of variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b0f02-6799-49a3-b67d-5b63a68eff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 25))\n",
    "cols = ['totalcases']  #'val_precipitation', 'val_temperature'] #, 'pop_near_water_10']\n",
    "sns.boxplot(data[data['totalcases'] > 0][cols], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989486b-9b1b-4df0-9d99-809272de5161",
   "metadata": {},
   "source": [
    "# Some more data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcba466-e461-4e01-a42d-358f768af307",
   "metadata": {},
   "source": [
    "# Machine Learning predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee3a04-509d-47a0-b584-34a8cad17d52",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626460db-0c31-4e43-b7b1-bbcc8f58dde7",
   "metadata": {},
   "source": [
    "### Use a decision tree to see how well the attributes can predict the total cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d70a2e-e3d4-4338-a7e0-a98a93b88337",
   "metadata": {},
   "source": [
    "Decide if we want to classify the cases. If we set n_classes = 1, the model will try to predict the actual case values. n_classes = 2 is a binary classification of either cases present or not. n_classes greater than 2 can be considered a range from low to high, but keeping 0 cases as its own class since 0 cases is an important cutoff for disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8881587-ed2d-4771-857d-f2843901c303",
   "metadata": {},
   "source": [
    "Note that I experimented with previous_week_totalcases and previous_week_neighbors_totalcases where I explicitly calcualted these values for each district and added them to the training data. I removed them since I feel it was not the correct thing to make it so explicit. Yes, the previous_week_totalcases did improve the models' predictions substantialy. However, the model also depended on this feature much more so than any other feature. And, in the real word, a model that relies almost completely on the previous weeks cases is not as valuable because we cannot control time and a model that focuses more on factors that we can control is best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d36901-0b2e-489e-b4f7-65c719c63816",
   "metadata": {},
   "source": [
    "* The n_classes variable is the number of classes for the machine learning problem.\n",
    "* n_classes = 1, that means do not classify the cases values and treat the problem as regression where you are trying to predict the actual number of cases.\n",
    "* n_classes = 2, is a binary classification of cases vs. no cases.\n",
    "* n_classes > 2, is a multiclass classifcation that will keep one of the classes as no cases. So, if you set it to 3, one class would be no classes and the other two would be low and high cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e2a2d-af4f-40d2-a8ed-79fcb4e15ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree for feature importance on a regression problem\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from matplotlib import pyplot\n",
    "import mapclassify as mc\n",
    "\n",
    "features_temp = [\n",
    "    'epidemic_week',\n",
    "    'val_precipitation',\n",
    "    'val_temperature',\n",
    "    'val_trees',\n",
    "    'percent_cov_trees',\n",
    "    'val_crops',\n",
    "    'percent_cov_crops',\n",
    "    'val_builtup',\n",
    "    'percent_cov_builtup',\n",
    "    'val_bareground',\n",
    "    'percent_cov_bareground',\n",
    "    'val_rangeland',\n",
    "    'percent_cov_rangeland',\n",
    "    'relative_pop_density',\n",
    "    'pop_near_water',\n",
    "    'val_wealth',\n",
    "    'val_elevation',\n",
    "    # 'previous_week_totalcases',\n",
    "    # 'previous_week_neighbors_totalcases',\n",
    "]\n",
    "\n",
    "features = [\n",
    "    'val_precipitation',\n",
    "    'val_temperature',\n",
    "    'trees',\n",
    "    'crops',\n",
    "    'builtup',\n",
    "    'bareground',\n",
    "    'rangeland',\n",
    "    'relative_pop_density',\n",
    "    'pop_near_water',\n",
    "    'val_wealth',\n",
    "    'val_elevation',\n",
    "    # 'previous_week_totalcases',\n",
    "    # 'previous_week_neighbors_totalcases',\n",
    "]\n",
    "\n",
    "label = 'cases'\n",
    "n_classes = 2\n",
    "\n",
    "if n_classes > 2:\n",
    "    nb = mc.NaturalBreaks(data[data['totalcases'] > 0]['totalcases'], k=n_classes-1) # I subtract 1 from the n_classes because I will consider 0 case counts as its own class and get that manually.\n",
    "\n",
    "def classify(d, bins):\n",
    "    c = []\n",
    "    for r in d:\n",
    "        v = 0\n",
    "        if r > 0:\n",
    "            for i in range(len(bins)-1, -1, -1):\n",
    "                if r > bins[i]:\n",
    "                    v = i + 2\n",
    "                    break\n",
    "                v = 1\n",
    "        c.append(v)\n",
    "    return np.array(c)\n",
    "\n",
    "\n",
    "def classify_binary(d):\n",
    "    c = []\n",
    "    for r in d:\n",
    "        v = 0\n",
    "        if r > 0:\n",
    "            v = 1\n",
    "        c.append(v)\n",
    "    return np.array(c)\n",
    "\n",
    "if n_classes > 2:\n",
    "    classes = classify(data['totalcases'], nb.bins-1)\n",
    "elif n_classes == 2:\n",
    "    classes = classify_binary(data['totalcases'])\n",
    "elif n_classes < 2:\n",
    "    classes = data['totalcases']\n",
    "data[label] = classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5392b94-e260-4c08-abe0-b00a10b1ea6d",
   "metadata": {},
   "source": [
    "### Select Previous weeks cases\n",
    "1. the number of cases for that district for the previous week\n",
    "2. the number of cases for the neighbors of that district and weight those cases by the Spatial Queen weighting above\n",
    "\n",
    "**Note**: be careful because on my computer which is a fairly solid computer, selecting the previous weeks cases **took about 5 hours.**\n",
    "\n",
    "Note that I excluded this code as described above."
   ]
  },
  {
   "cell_type": "raw",
   "id": "348ba270-2995-4a89-82b6-35f6bc083ceb",
   "metadata": {},
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "data['previous_week_totalcases'] = 0\n",
    "data['previous_week_neighbors_totalcases'] = 0.0\n",
    "neighbor_cases = []\n",
    "\n",
    "def get_previous_week_cases(my_id, epidemic_week):\n",
    "    previous_week = epidemic_week - np.timedelta64(1,'W')\n",
    "    previous_week_row = data[(data[\"my_id\"].values == my_id) & (data[\"epidemic_week\"].values == previous_week)]\n",
    "    if len(previous_week_row) > 0:\n",
    "        previous_total_cases = previous_week_row['totalcases'].tolist()[0]\n",
    "    else:\n",
    "        current_week_row = data[(data[\"my_id\"].values == my_id) & (data[\"epidemic_week\"].values == epidemic_week)]\n",
    "        previous_total_cases = current_week_row['totalcases'].tolist()[0]\n",
    "    return previous_total_cases\n",
    "\n",
    "\n",
    "def get_previous_week_neighbor_cases(my_id, epidemic_week):\n",
    "    previous_week = epidemic_week - np.timedelta64(1,'W')\n",
    "    my_neighbors = w_new.neighbors[my_id]\n",
    "    my_neighbors_cases_weighted = 0\n",
    "    for i in range(len(my_neighbors)):\n",
    "        n_my_id = my_neighbors[i]\n",
    "        previous_week_row = data[(data[\"my_id\"].values == n_my_id) & (data[\"epidemic_week\"].values == previous_week)]\n",
    "        if len(previous_week_row) > 0:\n",
    "            previous_neighbor_total_cases = previous_week_row['totalcases'].tolist()[0]\n",
    "            n_weight = w_new.weights[my_id][i]\n",
    "            previous_neighbor_total_cases_weighted = previous_neighbor_total_cases * n_weight\n",
    "            my_neighbors_cases_weighted = my_neighbors_cases_weighted + previous_neighbor_total_cases_weighted\n",
    "        else:\n",
    "            previous_week_row = data[(data[\"my_id\"].values == n_my_id) & (data[\"epidemic_week\"].values == epidemic_week)]\n",
    "            previous_neighbor_total_cases = previous_week_row['totalcases'].tolist()[0]\n",
    "            n_weight = w_new.weights[my_id][i]\n",
    "            previous_neighbor_total_cases_weighted = previous_neighbor_total_cases * n_weight\n",
    "            my_neighbors_cases_weighted = my_neighbors_cases_weighted + previous_neighbor_total_cases_weighted\n",
    "    return my_neighbors_cases_weighted"
   ]
  },
  {
   "cell_type": "raw",
   "id": "640730ce-5804-4463-93c7-f54467d41f80",
   "metadata": {},
   "source": [
    "my_ids = data['my_id'].to_numpy()\n",
    "my_weeks = data['epidemic_week'].to_numpy()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8dc7931c-feb4-4a89-af05-d015510cad9b",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "\n",
    "previous_week_totalcases = []\n",
    "previous_week_neighbors_totalcases = []\n",
    "\n",
    "s = time.time()\n",
    "for i in range(len(my_ids)):\n",
    "    previous_week_totalcases.append(get_previous_week_cases(my_ids[i], my_weeks[i]))\n",
    "    previous_week_neighbors_totalcases.append(get_previous_week_neighbor_cases(my_ids[i], my_weeks[i]))\n",
    "    \n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "304623a5-e222-47a8-8bd1-4a794c46cda4",
   "metadata": {},
   "source": [
    "data['previous_week_totalcases'] = previous_week_totalcases\n",
    "data['previous_week_neighbors_totalcases'] = previous_week_neighbors_totalcases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb33d3a-f06b-4d86-bc27-bfc91576627c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a7f35-7406-4768-b913-95b5082ef613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = data[features_temp]\n",
    "y = data[label]\n",
    "X_train_temp, X_test_temp, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d08707-0ce2-42c0-b05c-4740b38b7c11",
   "metadata": {},
   "source": [
    "### Create the landcover values for both the amount of coverage in the district and the percent of coverage of the district for each landcover class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fe6921-4fc4-4923-80c5-4a65659ad214",
   "metadata": {},
   "source": [
    "Combine measurements for:\n",
    "1. The amount of landcover\n",
    "2. The percent landcover\n",
    "3. The population density\n",
    "\n",
    "This was done so that large and small districts in area and those with high and low populations are not unrealistically affected by one of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceced67-1efe-4eff-8009-430a0e78781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "landcover_classes = [\n",
    "    'trees',\n",
    "    'crops',\n",
    "    'builtup',\n",
    "    'bareground',\n",
    "    'rangeland'\n",
    "]\n",
    "\n",
    "for lc in landcover_classes:\n",
    "    f1 = X_train_temp[f'val_{lc}']\n",
    "    f2 = X_train_temp[f'percent_cov_{lc}']\n",
    "    f3 = X_train_temp['relative_pop_density']\n",
    "    f1_min = f1.min()\n",
    "    f1_max = f1.max()\n",
    "    f2_min = f2.min()\n",
    "    f2_max = f2.max()\n",
    "    f3_min = f3.min()\n",
    "    f3_max = f3.max()\n",
    "    X_train_temp[lc] = ((((f1-f1_min) / (f1_max-f1_min)) + ((f2-f2_min) / (f2_max-f2_min))) / 2) * ((f3-f3_min) / (f3_max-f3_min))\n",
    "    \n",
    "    f1 = X_test_temp[f'val_{lc}']\n",
    "    f2 = X_test_temp[f'percent_cov_{lc}']\n",
    "    f3 = X_test_temp['relative_pop_density']\n",
    "    f1_min = f1.min()\n",
    "    f1_max = f1.max()\n",
    "    f2_min = f2.min()\n",
    "    f2_max = f2.max()\n",
    "    f3_min = f3.min()\n",
    "    f3_max = f3.max()\n",
    "    X_test_temp[lc] = ((((f1-f1_min) / (f1_max-f1_min)) + ((f2-f2_min) / (f2_max-f2_min))) / 2) * ((f3-f3_min) / (f3_max-f3_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeae2654-37aa-4dc8-981d-21d219d1bd85",
   "metadata": {},
   "source": [
    "### Select the features to use in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c6692-2693-4763-97d1-1d22a68e98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_temp[features]\n",
    "X_test = X_test_temp[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966887f-b668-43f8-b270-335edb54a8a6",
   "metadata": {},
   "source": [
    "### Normalize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89c4d9-0a33-47c4-bf77-30033a9afd53",
   "metadata": {},
   "source": [
    "Use the RobustScaler because our data has lots of outliers and the RobustScaler better normalizes when there are many outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b997206-d30f-4639-96ba-9f0a1ab453b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "scaler = scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb54fe-f580-464d-8201-c7d7677fde5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns), X_train_temp['epidemic_week']], axis=1)\n",
    "X_test = pd.concat([pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns), X_test_temp['epidemic_week']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8414a09-cc8d-495e-8b3c-fd36dd9b994f",
   "metadata": {},
   "source": [
    "### Perform oversampling or undersampling if desired.\n",
    "\n",
    "I excluded this in the end. Most AutoML libraries include ways to account for class imbalances without the need for oversampling or undersampling. And testing with both random undersampling and SMOTE oversampling the results were worse. Therefore, I preferred to train the model with eval_metric='f1' because it can utilize all data and still try to account for the class imbalanace to produce the best f1 score which optimizes both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47485a7a-e7b0-435b-b415-cc35cc9b8ae7",
   "metadata": {},
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "\n",
    "n_jobs = os.cpu_count() - 2\n",
    "resampling_methods = {\n",
    "    'NearMiss': NearMiss(n_jobs=n_jobs),\n",
    "    'RandomOverSampler': RandomOverSampler(),\n",
    "    'ADASYN': ADASYN(n_jobs=n_jobs), \n",
    "    'SVMSMOTE': SVMSMOTE(n_jobs=n_jobs),\n",
    "    'KMeansSMOTE': KMeansSMOTE(n_jobs=n_jobs),\n",
    "    'SMOTEENN': SMOTEENN(n_jobs=n_jobs),\n",
    "    'SMOTETomek': SMOTETomek(n_jobs=n_jobs)\n",
    "}\n",
    "\n",
    "def perform_resampling(X, y, resampling_method):\n",
    "    #input DataFrame\n",
    "    #X →Independent Variable in DataFrame\\\n",
    "    #y →dependent Variable in Pandas DataFrame format\n",
    "    X, y = resampling_method.fit_resample(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f885fc1-9e1c-4ea4-86a7-8445addd981b",
   "metadata": {
    "tags": []
   },
   "source": [
    "X_train, y_train = perform_resampling(X_train, y_train, resampling_methods['ADASYN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1019eb89-bb6e-46b7-86f0-aca2ae53ec65",
   "metadata": {},
   "source": [
    "### Put the features and labels together into one dataframe with the labels as the last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc4572c-8e07-4b21-bb55-1c2ef6fbeabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc420ece-7704-41cd-a430-390d643d554a",
   "metadata": {},
   "source": [
    "### Set up AutoML using AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa499357-4e78-430c-b642-f58549a22bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "train_data = TabularDataset(df_train)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a7ee34-d039-467d-976a-e05b3c59dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'cases'\n",
    "save_path = 'agModels-predictClass'  # specifies folder to store trained models\n",
    "predictor = TabularPredictor(label=label, path=save_path, sample_weight='auto_weight', eval_metric='f1')   # eval_metric='recall_weighted' for more than 2 classes # recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca4dbe9-f4f0-4eac-a1eb-5e090c1b76ef",
   "metadata": {},
   "source": [
    "### Train (fit) the model\n",
    "\n",
    "If you delete the time_limit parameter or set it higher, it will obviously run longer but you will most likely get better results. From my experience, when you are developing, testing different scenarios, or changing lots of code above, then set the time_limit=180 or 180 seconds. You will likely get decent results. Then, once your code is more set, you can run it longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c08ce5-f1e1-4ccd-acaa-ff2e9ff1b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = predictor.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941cf210-d154-4685-86e7-a8f004b5e20a",
   "metadata": {},
   "source": [
    "### Evaluate the best model created by AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5063af3-ab5f-4fc6-83d7-b98ae42c7a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TabularDataset(df_test)\n",
    "\n",
    "y_pred = predictor.predict(test_data.drop(columns=[label]))\n",
    "\n",
    "df_test[f'{label}_pred'] = y_pred\n",
    "# df_test.to_pickle('df_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807093f-828f-406b-98a2-9b9ce49db733",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtr = predictor.evaluate(test_data, silent=True)\n",
    "model_metrics_file = f'{report_image_path}{os.sep}{ts}_{n_classes}_modelmetrics.csv'\n",
    "file_exists = os.path.isfile(model_metrics_file)\n",
    "with open(model_metrics_file, 'a') as f:\n",
    "    if n_classes == 2:\n",
    "        if not file_exists:\n",
    "            f.write(\"disease,accuracy,balanced_accuracy,mcc,roc_auc,f1,precision,recall\\n\")\n",
    "        f.write(f\"{disease},{round(mtr['accuracy'],3)},{round(mtr['balanced_accuracy'],3)},{round(mtr['mcc'],3)},{round(mtr['roc_auc'],3)},{round(mtr['f1'],3)},{round(mtr['precision'],3)},{round(mtr['recall'],3)}\\n\")\n",
    "    if n_classes == 3:\n",
    "        if not file_exists:\n",
    "            f.write(\"disease,accuracy,balanced_accuracy,recall_weighted,mcc\\n\")\n",
    "        f.write(f\"{disease},{round(mtr['accuracy'],3)},{round(mtr['balanced_accuracy'],3)},{round(mtr['recall_weighted'],3)},{round(mtr['mcc'],3)}\\n\")\n",
    "mtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e174e2-1dec-45b8-96cd-1335f46f2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix\n",
    "if n_classes == 2:\n",
    "    cm = ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=['no cases', 'cases'], values_format = '.6g', normalize='true')\n",
    "    cm.figure_.savefig(f'{report_image_path}{os.sep}{ts}_{disease}_{n_classes}_confusion_matrix.png', bbox_inches='tight', transparent=True, dpi=300)\n",
    "if n_classes == 3:\n",
    "    cm = ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=['none', 'low', 'high'], values_format = '.6g', normalize='true')\n",
    "    cm.figure_.savefig(f'{report_image_path}{os.sep}{ts}_{disease}_{n_classes}_confusion_matrix.png', bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0f9b1-2841-489a-b317-628daec9ff1e",
   "metadata": {},
   "source": [
    "### List the best performing models from AutoGluon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbd940-76a1-492c-a5cb-8fa188cf2e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard(test_data, silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0724b9f-1d94-4903-b474-87aa5c3819f8",
   "metadata": {},
   "source": [
    "## Generate the feature importances.\n",
    "\n",
    "The article below describes how AutoGluon calculates feature importance.\n",
    "\n",
    "https://explained.ai/rf-importance/\n",
    "\n",
    "It is important to note that feature importance is similar to a variables multiplier in linear regression, but it is not the same and calculated differently. Feature importance, gives you a good sense of which variables are most important to the model making predictions. Therefore, it is not exactly the same as which variable is most important to determining disease spread, but it gives you a very close estimate to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d7551-a5c4-4d47-9982-ee852a3b79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = predictor.feature_importance(train_data)\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa5ada2-ff24-438c-b65c-e5da9e015fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.barh(y=fi.index, width=fi.importance)\n",
    "ax.invert_yaxis()\n",
    "plt.grid()\n",
    "plt.savefig(f'{report_image_path}{os.sep}{ts}_{disease}_{n_classes}_featureimportancechart.png', bbox_inches='tight', transparent=True, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf921d-7206-4555-99e9-e6451b545c25",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Note**: the code below is **deprecated** because I switched to use the AutoGluon library above for AutoML instead of FLAML below. AutoGluon is getting better results and it has a few built in deep learning models to choose from.\n",
    "\n",
    "### Set up AutoML using Microsoft's FLAML library\n",
    "\n",
    "AutoML will help us top pick the best performing model for predicting the data.\n",
    "\n",
    "If n_classes >= 2, then we should treat it as a classifcation problem. Otherwise, predicting the actual case numbers is a regression problem."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5aa542dd-820a-4326-ae09-c54888d408f9",
   "metadata": {},
   "source": [
    "from sklearn import set_config\n",
    "from flaml import AutoML\n",
    "\n",
    "set_config(display='diagram')\n",
    "\n",
    "automl = AutoML()\n",
    "automl_settings = {\n",
    "    \"time_budget\": 300,  # in seconds\n",
    "    \"early_stop\": True,\n",
    "    \"log_file_name\": f\"casesclass_{disease}.log\"\n",
    "}\n",
    "\n",
    "if n_classes >= 2:\n",
    "    automl_settings['task'] = 'classification'\n",
    "    automl_settings['metric'] = 'roc_auc_weighted'\n",
    "    automl_settings['estimator_list'] = ['lgbm', 'xgboost', 'xgb_limitdepth', 'rf', 'extra_tree', 'catboost']\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    unsorted_list = [(uni, count) for uni, count in \n",
    "                  zip(unique, counts)]\n",
    "    sorted_list = sorted(unsorted_list)\n",
    "    unique = sorted_list[0]\n",
    "    counts = sorted_list[1]\n",
    "    sample_weight = y_train.copy().astype(float)\n",
    "    for l in sorted_list:\n",
    "        v = l[0]\n",
    "        w = 1-(l[1]/len(y_train))\n",
    "        sample_weight[sample_weight == float(v)] = w\n",
    "    automl_settings['sample_weight'] = sample_weight\n",
    "else:\n",
    "    automl_settings['task'] = 'regression'\n",
    "    automl_settings['metric'] = 'r2'\n",
    "\n",
    "pipeline_settings = {\n",
    "    f\"automl__{key}\": value for key, value in automl_settings.items()\n",
    "}\n",
    "\n",
    "steps = [('automl', automl)]\n",
    "automl_pipeline = Pipeline(steps)\n",
    "automl_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e283794-ce3e-44a3-9e27-6c103b8b5cc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train the model using AutoML"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0439af7-cdf6-4a75-8f05-f38109fc467c",
   "metadata": {},
   "source": [
    "%%capture\n",
    "\n",
    "automl_pipeline.fit(X_train, y_train, **pipeline_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dd9a95-677a-46ec-a3c1-606b9fdf5cc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Find the best performing model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "273c94c0-4bc9-4791-bac9-605613aabbdf",
   "metadata": {},
   "source": [
    "automl = automl_pipeline.steps[0][1]\n",
    "\n",
    "print('Best hyperparmeter config:', automl.best_config)\n",
    "print('Best metric on validation data: {0:.4g}'.format(1-automl.best_loss))\n",
    "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))\n",
    "print(automl.model.estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee130cb0-2000-4e24-bfa6-393fcc3d3f4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute predictions of testing dataset\n",
    "\n",
    "This is to get a first look at how the model did."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5f9d502-1a7e-4df6-b450-969c8fd1c172",
   "metadata": {
    "tags": []
   },
   "source": [
    "y_pred = automl_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c5eca-b6d7-46c6-ae2c-3b1f2aafb2bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute different metric values on testing dataset to evalute it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6839955-36b5-4520-b3c6-4dde9cf65083",
   "metadata": {},
   "source": [
    "#### metrics\n",
    "\n",
    "If its a regresssion problem then we need to use different metrics compared to a classification problem."
   ]
  },
  {
   "cell_type": "raw",
   "id": "706dfc8e-3d7b-4afb-a2b7-1c4217650b5b",
   "metadata": {},
   "source": [
    "if n_classes >= 2:\n",
    "    print('f1', '=', f1_score(y_test, y_pred, average='binary', pos_label=1))\n",
    "    print('accuracy', '=', accuracy_score(y_test, y_pred))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=['no cases', 'cases'], values_format = '.5g')\n",
    "    print('classification report', classification_report(y_test, y_pred, target_names=['no cases', 'cases']))\n",
    "else:\n",
    "    print('r2', '=', r2_score(y_test, y_pred))\n",
    "    print('rmse', '=', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    print('mae', '=', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d9a25-76ae-496b-8d56-b6163d6039e9",
   "metadata": {},
   "source": [
    "### compare predicted versus actual\n",
    "\n",
    "Let us print some of the predictions compared to the actual values to visually check the predictions.\n",
    "In the classification problem, I subsetted the values where there are actual cases present since this is the smaller class and therefore more challenging to get correct."
   ]
  },
  {
   "cell_type": "raw",
   "id": "493c43d6-0b4f-4454-8615-4936d0c5a2a5",
   "metadata": {},
   "source": [
    "if n_classes >= 2:\n",
    "    idxs = np.where(y_test >= 1)\n",
    "    print(y_pred[idxs][:500])\n",
    "    print(y_test[idxs][:500])\n",
    "else:\n",
    "    idxs = np.where(y_test >= 1)\n",
    "    print(y_pred[idxs][:500].astype(int))\n",
    "    print(y_test[idxs][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e9079-8bab-4b2e-b309-112d83771efb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot the learning curve\n",
    "\n",
    "To make sure we avoid overfitting and to see if training longer will produce better results."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dc78fc8-3c38-4c50-a6d7-cb185f4be779",
   "metadata": {},
   "source": [
    "from flaml.data import get_output_from_log\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "\n",
    "time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history = \\\n",
    "    get_output_from_log(filename=automl_settings['log_file_name'], time_budget=60)\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Wall Clock Time (s)')\n",
    "plt.ylabel('Validation metric')\n",
    "plt.step(time_history, 1 - np.array(best_valid_loss_history), where='post')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c7ad2-61ac-482e-a26d-46b430e56b90",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot feature importance\n",
    "\n",
    "The feature importance measures tells us the variables that the model decided are most important to predicting disease cases."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4d55796-b1e1-49c5-a1c2-085a2f118a60",
   "metadata": {},
   "source": [
    "unsorted_list = [(importance, feature) for feature, importance in \n",
    "                  zip(features, automl.feature_importances_)]\n",
    "sorted_list = sorted(unsorted_list)\n",
    "features_sorted = []\n",
    "importance_sorted = []\n",
    "\n",
    "for i in sorted_list:\n",
    "    features_sorted += [i[1]]\n",
    "    importance_sorted += [i[0]]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.barh(features_sorted, importance_sorted)\n",
    "plt.savefig(f'results/20230704_{disease}_featureimportance.png', dpi=150, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
